{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jax tensorflow tf-keras tensorflow_probability matplotlib seaborn scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_prob = 0.1, Estimated Stdev: 0.030739184736597532\n",
      "base_prob = 0.2, Estimated Stdev: 0.06127379077852825\n",
      "base_prob = 0.3, Estimated Stdev: 0.09242769914742267\n",
      "base_prob = 0.4, Estimated Stdev: 0.12239545694049353\n",
      "base_prob = 0.5, Estimated Stdev: 0.15058367202251338\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for base_prob in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "\n",
    "    # Number of samples for simulation\n",
    "    num_samples = 10000\n",
    "\n",
    "    # Generate samples for c\n",
    "    c_samples = np.random.uniform(0, 1, num_samples)\n",
    "\n",
    "    # Compute the expression for each sample\n",
    "    values = np.clip(base_prob + base_prob * np.sin(c_samples * np.pi), 0.01, 0.99)\n",
    "\n",
    "    # Calculate the variance of the resulting values\n",
    "    variance = np.var(values)\n",
    "\n",
    "    print(f\"base_prob = {base_prob}, Estimated Stdev: {variance**0.5}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single pass model fails to capture epistemic uncertainty !\n",
    "ie it doesn't know that it is looking at OOD data ! \n",
    "It knows the aleatoric uncertainty which is the irreducible noise in the data but it does not know about unseen training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"BetaBernoulliModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"BetaBernoulliModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_95      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_305 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ input_layer_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_306 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dense_305[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ alpha (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_306[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ beta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_306[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_209 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ alpha[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_210 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ beta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_46      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_209[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ add_210[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_95      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_305 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m384\u001b[0m │ input_layer_95[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_306 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ dense_305[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ alpha (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_306[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ beta (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_306[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_209 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ alpha[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_210 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ beta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_46      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ add_209[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ add_210[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,674</span> (18.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,674\u001b[0m (18.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,674</span> (18.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,674\u001b[0m (18.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1856 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2273\n",
      "Epoch 2/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1756 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2264\n",
      "Epoch 3/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1775 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2498\n",
      "Epoch 4/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1780 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2287\n",
      "Epoch 5/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1677 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2468\n",
      "Epoch 6/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1786 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2363\n",
      "Epoch 7/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1793 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2435\n",
      "Epoch 8/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1720 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2439\n",
      "Epoch 9/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1724 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2257\n",
      "Epoch 10/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - beta_bernoulli_accuracy: 0.5681 - loss: 1.1685 - val_beta_bernoulli_accuracy: 0.3469 - val_loss: 1.2468\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"monte_carlo_dropout\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"monte_carlo_dropout\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_96      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_307 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ input_layer_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_308 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dense_307[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mc_dropout_38       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_308[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MCDropout</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_211 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mc_dropout_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ dense_307[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_309 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ add_211[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_96      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_307 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m384\u001b[0m │ input_layer_96[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_308 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ dense_307[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mc_dropout_38       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_308[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMCDropout\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_211 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ mc_dropout_38[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ dense_307[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_309 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ add_211[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,609</span> (18.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,609\u001b[0m (18.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,609</span> (18.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,609\u001b[0m (18.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5985 - loss: 0.6677 - val_accuracy: 0.6600 - val_loss: 0.5950\n",
      "Epoch 2/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7919 - loss: 0.4995 - val_accuracy: 0.6640 - val_loss: 0.5837\n",
      "Epoch 3/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7919 - loss: 0.4953 - val_accuracy: 0.6651 - val_loss: 0.5805\n",
      "Epoch 4/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7919 - loss: 0.4934 - val_accuracy: 0.6663 - val_loss: 0.5785\n",
      "Epoch 5/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7919 - loss: 0.4909 - val_accuracy: 0.6655 - val_loss: 0.5765\n",
      "Epoch 6/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7919 - loss: 0.4881 - val_accuracy: 0.6648 - val_loss: 0.5754\n",
      "Epoch 7/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7919 - loss: 0.4845 - val_accuracy: 0.6698 - val_loss: 0.5695\n",
      "Epoch 8/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7919 - loss: 0.4850 - val_accuracy: 0.6679 - val_loss: 0.5700\n",
      "Epoch 9/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7921 - loss: 0.4837 - val_accuracy: 0.6674 - val_loss: 0.5676\n",
      "Epoch 10/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7915 - loss: 0.4794 - val_accuracy: 0.6669 - val_loss: 0.5661\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m1565/1565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow.keras.layers import Input, Add, Dropout, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "num_layers = 2\n",
    "num_units = 64\n",
    "dropout_rate = 0.25\n",
    "num_dropout_samples = 128\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "data_size = 10000\n",
    "\n",
    "class MCDropout(Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "# True probabilities based on feature combinations\n",
    "true_class_prob_var = {\n",
    "        ('A', 'X'): {\"prob\" : 0.1, \"stdev\" : 0.030952139346463268} ,\n",
    "        ('A', 'Y'): {\"prob\" : 0.2, \"stdev\" : 0.06119698957156734},\n",
    "        ('B', 'X'): {\"prob\": 0.3, \"stdev\": 0.09229488469895951},\n",
    "        ('B', 'Y'): {\"prob\": 0.5, \"stdev\": 0.15374131852930115}\n",
    "    }\n",
    "\n",
    "def true_prob(c1, c2, c):\n",
    "    base_prob = true_class_prob_var[(c1, c2)][\"prob\"]\n",
    "    return np.clip(base_prob + base_prob * np.sin(c * np.pi), 0.01, 0.99)\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(n_samples=data_size):\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B'], n_samples)\n",
    "    cat2 = np.random.choice(['X', 'Y'], n_samples)\n",
    "    \n",
    "    # Continuous feature\n",
    "    cont = np.random.uniform(0, 1, n_samples)\n",
    "    \n",
    "    probs = np.array([true_prob(c1, c2, c) for c1, c2, c in zip(cat1, cat2, cont)])\n",
    "    vars = np.array([true_prob(c1, c2, c) for c1, c2, c in zip(cat1, cat2, cont)])\n",
    "    # Generate target variable with aleatoric uncertainty\n",
    "    y = np.random.binomial(1, probs)\n",
    "    \n",
    "    return (cat1, cat2, cont), y, probs, vars\n",
    "\n",
    "# Generate data\n",
    "(cat1, cat2, cont), y, true_probs, true_vars = generate_data()\n",
    "\n",
    "# Separate in-distribution (A) and out-of-distribution (B) data\n",
    "mask_in_dist = (cat1 == 'A') | ((cat1 == 'B') & (cat2 == 'Y'))\n",
    "mask_out_dist = (cat1 == 'B') & (cat2 == 'X')\n",
    "\n",
    "# In-distribution data (A)\n",
    "cat1_in = cat1[mask_in_dist]\n",
    "cat2_in = cat2[mask_in_dist]\n",
    "cont_in = cont[mask_in_dist]\n",
    "y_in = y[mask_in_dist]\n",
    "\n",
    "# Out-of-distribution data (B)\n",
    "cat1_out = cat1[mask_out_dist]\n",
    "cat2_out = cat2[mask_out_dist]\n",
    "cont_out = cont[mask_out_dist]\n",
    "y_out = y[mask_out_dist]\n",
    "\n",
    "# Preprocess in-distribution data\n",
    "cat1_in_encoded = tf.keras.utils.to_categorical(np.searchsorted(['A', 'B'], cat1_in), num_classes=2)\n",
    "cat2_in_encoded = tf.keras.utils.to_categorical(np.searchsorted(['X', 'Y'], cat2_in), num_classes=2)\n",
    "X_in = np.hstack([cat1_in_encoded, cat2_in_encoded, cont_in.reshape(-1, 1)])\n",
    "\n",
    "# Preprocess out-of-distribution data\n",
    "cat1_out_encoded = tf.keras.utils.to_categorical(np.searchsorted(['A', 'B'], cat1_out), num_classes=2)\n",
    "cat2_out_encoded = tf.keras.utils.to_categorical(np.searchsorted(['X', 'Y'], cat2_out), num_classes=2)\n",
    "X_out = np.hstack([cat1_out_encoded, cat2_out_encoded, cont_out.reshape(-1, 1)])\n",
    "\n",
    "# Split in-distribution data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in, y_in, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_beta_bernoulli_model():\n",
    "    inputs = Input(shape=(5,))\n",
    "    x = Dense(num_units, activation='selu')(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        residual = x\n",
    "        x = Dense(num_units, activation='selu')(x)\n",
    "        # x = Add()([x, residual])\n",
    "    alpha = Dense(1, activation='softplus', name='alpha')(x) + 1.0\n",
    "    beta = Dense(1, activation='softplus', name='beta')(x) + 1.0\n",
    "    outputs = Concatenate()([alpha, beta])\n",
    "    return Model(inputs=inputs, outputs=outputs, name='BetaBernoulliModel')\n",
    "\n",
    "# Custom loss function (negative log likelihood of Beta-Bernoulli distribution)\n",
    "def nll_beta_bernoulli(y_true, y_pred):\n",
    "    alpha = y_pred[:, 0]\n",
    "    beta = y_pred[:, 1]\n",
    "    tfd = tfp.distributions\n",
    "    # Define the Beta distribution\n",
    "    beta_dist = tfd.Beta(concentration1=alpha, concentration0=beta)\n",
    "    # Sample p from the Beta distribution\n",
    "    p = beta_dist.sample()\n",
    "    # Define the Bernoulli distribution with the sampled p\n",
    "    bernoulli_dist = tfd.Bernoulli(probs=p)\n",
    "    # Compute the log likelihood\n",
    "    log_likelihood = bernoulli_dist.log_prob(y_true)\n",
    "    # Compute the negative log likelihood\n",
    "    nll = -tf.reduce_mean(log_likelihood)\n",
    "    # Regularization term to ensure alpha and beta do not explode\n",
    "    regularization = 1e-1 * (alpha + beta)\n",
    "    return nll + regularization\n",
    "\n",
    "# Custom accuracy metric for Beta-Bernoulli model\n",
    "def beta_bernoulli_accuracy(y_true, y_pred):\n",
    "    alpha = y_pred[:, 0]\n",
    "    beta = y_pred[:, 1]\n",
    "    mean = alpha / (alpha + beta)\n",
    "    return tf.keras.metrics.binary_accuracy(y_true, mean)\n",
    "\n",
    "def create_model_with_dropout():\n",
    "    inputs = Input(shape=(5,))\n",
    "    x = Dense(num_units, activation='relu')(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        residual = x\n",
    "        x = Dense(num_units, activation='relu')(x)\n",
    "        x = MCDropout(dropout_rate)(x)\n",
    "        x = Add()([x, residual])\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='monte_carlo_dropout')\n",
    "    return model\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create and compile the single pass uncertainty estimation model\n",
    "model_single_pass = create_beta_bernoulli_model()\n",
    "model_single_pass.compile(loss=nll_beta_bernoulli, optimizer='adam',  metrics=[beta_bernoulli_accuracy])\n",
    "model_single_pass.summary()\n",
    "model_single_pass.fit(X_train, y_train.reshape(-1, 1), validation_data=(X_val, y_val.reshape(-1, 1)), batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Create and compile the multi-pass uncertainty estimation model\n",
    "model_multi_pass = create_model_with_dropout()\n",
    "model_multi_pass.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_multi_pass.summary()\n",
    "model_multi_pass.fit(X_train, y_train.reshape(-1, 1), validation_data=(X_val, y_val.reshape(-1, 1)), epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Predict mean and variance for single pass model\n",
    "def predict_single_pass(model, X):\n",
    "    if X.shape[1] != model.input_shape[1]:\n",
    "        raise ValueError(f\"Input shape mismatch: expected {model.input_shape[1]}, got {X.shape[1]}\")\n",
    "    y_pred = model.predict(X)\n",
    "    alpha = y_pred[:, 0]\n",
    "    beta = y_pred[:, 1]\n",
    "    mean = alpha / (alpha + beta)\n",
    "    variance = alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))\n",
    "    assert mean.shape == variance.shape\n",
    "    return mean, variance\n",
    "\n",
    "# Predict mean and variance for multi-pass model using Monte Carlo Dropout\n",
    "def predict_multi_pass(model, X, n_samples=num_dropout_samples):\n",
    "    # Define a function to perform a single prediction with dropout\n",
    "    @tf.function\n",
    "    def single_predict():\n",
    "        return model(X, training=True)\n",
    "    \n",
    "    # Perform multiple forward passes\n",
    "    preds = tf.stack([single_predict() for _ in range(n_samples)])\n",
    "    \n",
    "    # Compute the mean and variance\n",
    "    mean = tf.reduce_mean(preds, axis=0)\n",
    "    variance = tf.math.reduce_variance(preds, axis=0)\n",
    "    \n",
    "    return mean.numpy(), variance.numpy()\n",
    "\n",
    "# Get predictions\n",
    "mean_single_in, var_single_in = predict_single_pass(model_single_pass, X_train)\n",
    "mean_single_out, var_single_out = predict_single_pass(model_single_pass, X_val)\n",
    "mean_multi_in, var_multi_in = predict_multi_pass(model_multi_pass, X_train)\n",
    "mean_multi_out, var_multi_out = predict_multi_pass(model_multi_pass, X_val)\n",
    "\n",
    "def decode_one_hot(encoded_vector):\n",
    "    cat1_label = 'A' if encoded_vector[0] == 1 else 'B'\n",
    "    cat2_label = 'X' if encoded_vector[2] == 1 else 'Y'\n",
    "    return f\"{cat1_label},{cat2_label}\"\n",
    "\n",
    "# Generate class labels for input data\n",
    "class_labels_in = [decode_one_hot(x) for x in X_train]\n",
    "class_labels_out = [decode_one_hot(x) for x in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[276], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m cont_out \u001b[38;5;241m=\u001b[39m X_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cont_in) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(mean_single_in) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(var_single_in) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_labels_in)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cont_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(mean_single_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(var_single_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_labels_out)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plotting function with class labels\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_results\u001b[39m(cont_in, mean_in, var_in, cont_out, mean_out, var_out, class_labels_in, class_labels_out, title):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure the lengths of the arrays match\n",
    "cont_in = X_train[:, -1]\n",
    "cont_out = X_out[:, -1]\n",
    "assert len(cont_in) == len(mean_single_in) == len(var_single_in) == len(class_labels_in)\n",
    "assert len(cont_out) == len(mean_single_out) == len(var_single_out) == len(class_labels_out)\n",
    "\n",
    "# Plotting function with class labels\n",
    "def plot_results(cont_in, mean_in, var_in, cont_out, mean_out, var_out, class_labels_in, class_labels_out, title):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # In-distribution Mean\n",
    "    for label in np.unique(class_labels_in):\n",
    "        mask = np.array(class_labels_in) == label\n",
    "        axs[0, 0].scatter(cont_in[mask], mean_in[mask], s=1, label=f'{label}')\n",
    "    axs[0, 0].set_title(f'{title} - In-distribution Mean')\n",
    "    axs[0, 0].set_xlabel('Continuous Variable')\n",
    "    axs[0, 0].set_ylabel('Mean')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Out-of-distribution Mean\n",
    "    for label in np.unique(class_labels_out):\n",
    "        mask = np.array(class_labels_out) == label\n",
    "        axs[0, 1].scatter(cont_out[mask], mean_out[mask], s=1, label=f'{label}')\n",
    "    axs[0, 1].set_title(f'{title} - Out-of-distribution Mean')\n",
    "    axs[0, 1].set_xlabel('Continuous Variable')\n",
    "    axs[0, 1].set_ylabel('Mean')\n",
    "    axs[0, 1].legend()\n",
    "    \n",
    "    # In-distribution Variance\n",
    "    for label in np.unique(class_labels_in):\n",
    "        mask = np.array(class_labels_in) == label\n",
    "        axs[1, 0].scatter(cont_in[mask], var_in[mask], s=1, label=f'{label}')\n",
    "    axs[1, 0].set_title(f'{title} - In-distribution Variance')\n",
    "    axs[1, 0].set_xlabel('Continuous Variable')\n",
    "    axs[1, 0].set_ylabel('Variance')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # Out-of-distribution Variance\n",
    "    for label in np.unique(class_labels_out):\n",
    "        mask = np.array(class_labels_out) == label\n",
    "        axs[1, 1].scatter(cont_out[mask], var_out[mask], s=1, label=f'{label}')\n",
    "    axs[1, 1].set_title(f'{title} - Out-of-distribution Variance')\n",
    "    axs[1, 1].set_xlabel('Continuous Variable')\n",
    "    axs[1, 1].set_ylabel('Variance')\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for single pass model\n",
    "plot_results(cont_in, mean_single_in, var_single_in, cont_out, mean_single_out, var_single_out, class_labels_in, class_labels_out, 'Single Pass Model')\n",
    "\n",
    "# Plot results for multi-pass model\n",
    "plot_results(cont_in, mean_multi_in, var_multi_in, cont_out, mean_multi_out, var_multi_out, class_labels_in, class_labels_out, 'Multi Pass Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need better data, more realistic data, maybe the continuous feature can be the axis on which the OOD happens instead of the categoricals because we can vary the continuous as much as we want\n",
    "\n",
    "make the OOD data follow a radically different distribution\n",
    "\n",
    "make the bayesian network work\n",
    "\n",
    "\n",
    "\n",
    "REssources : \n",
    "https://github.com/clabrugere/evidential-deeplearning/tree/main\n",
    "https://arxiv.org/pdf/1806.01768\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
