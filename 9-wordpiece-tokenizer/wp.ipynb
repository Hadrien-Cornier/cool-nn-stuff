{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of WordPiece is that we will consider the following ratio to be the guide for the next tokens to merge : \n",
    "\n",
    "$$R = \\frac{f(AB)}{f(A)f(B)}$$\n",
    "\n",
    "where $f$  is the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing_extensions import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = \"lorem ipsum dolor sit amet consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = training_corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_word = lambda word: [word[0]] + [\"##\" + word[i] for i in range(1, len(word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lorem'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is how we build the original vocaubulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['l', '##o', '##r', '##e', '##m']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"this is how we build the original vocaubulary\")\n",
    "explode_word(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_map(encoded_training_corpus: list) -> dict:\n",
    "    freq_map = {}\n",
    "    for word in encoded_training_corpus:\n",
    "        for token in word:\n",
    "            freq_map[token] = freq_map.get(token, 0) + 1\n",
    "    return freq_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(training_corpus: str) -> dict:\n",
    "    words = training_corpus.split()\n",
    "    return [explode_word(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['l', '##o', '##r', '##e', '##m'], ['i', '##p', '##s', '##u', '##m']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = encode(training_corpus)\n",
    "E[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', 4), ('##o', 27), ('##r', 21), ('##e', 27), ('##m', 14)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = freq_map(E)\n",
    "list(F.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'v', '##a', '##n', 's']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set().union(*[set(word) for word in E])\n",
    "list(vocab)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need $f$ frequency for each individual token, so that we can compute the denominator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('l', '##o'), ('##o', '##r'), ('##r', '##e'), ('##e', '##m')],\n",
       " [('i', '##p'), ('##p', '##s'), ('##s', '##u'), ('##u', '##m')]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_bigrams(L:list) -> list:\n",
    "    return [list(zip(sub, sub[1:])) for sub in L]\n",
    "tuples = make_bigrams(E)\n",
    "tuples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##a+b = ##ab\n"
     ]
    }
   ],
   "source": [
    "def generate_merge_token(p,s):\n",
    "    if p.startswith(\"##\"):\n",
    "        return p + \"+\" + s + \" = ##\"+ (p + s).replace(\"#\",\"\")\n",
    "    return p + \"+\" + s + \" = \" + p + s.replace(\"#\",\"\")\n",
    "\n",
    "print(generate_merge_token(\"##a\",\"b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are looking for which contiguous tokens we can merge\n",
      "frequency of merges :  {'l+##o = lo': 1, '##o+##r = ##or': 9, '##r+##e = ##re': 6, '##e+##m = ##em': 2, 'i+##p = ip': 1, '##p+##s = ##ps': 1, '##s+##u = ##su': 1, '##u+##m = ##um': 3, 'd+##o = do': 5, '##o+##l = ##ol': 6, '##l+##o = ##lo': 4, 's+##i = si': 2, '##i+##t = ##it': 6, 'a+##m = am': 1, '##m+##e = ##me': 1, '##e+##t = ##et': 2, 'c+##o = co': 3, '##o+##n = ##on': 4, '##n+##s = ##ns': 2, '##s+##e = ##se': 4, '##e+##c = ##ec': 2, '##c+##t = ##ct': 1, '##t+##e = ##te': 4, '##t+##u = ##tu': 2, '##u+##r = ##ur': 4, 'a+##d = ad': 2, '##d+##i = ##di': 2, '##i+##p = ##ip': 2, '##p+##i = ##pi': 2, '##i+##s = ##is': 5, '##s+##c = ##sc': 1, '##c+##i = ##ci': 4, '##i+##n = ##in': 3, '##n+##g = ##ng': 1, 'e+##l = el': 1, '##l+##i = ##li': 5, 's+##e = se': 1, '##e+##d = ##ed': 1, 'e+##i = ei': 1, '##i+##u = ##iu': 1, '##u+##s = ##us': 1, '##s+##m = ##sm': 1, '##m+##o = ##mo': 2, '##o+##d = ##od': 2, 't+##e = te': 1, '##m+##p = ##mp': 1, '##p+##o = ##po': 1, 'i+##n = in': 4, '##n+##c = ##nc': 1, '##i+##d = ##id': 4, '##d+##u = ##du': 1, '##u+##n = ##un': 3, '##n+##t = ##nt': 5, 'u+##t = ut': 3, 'l+##a = la': 3, '##a+##b = ##ab': 3, '##b+##o = ##bo': 3, 'e+##t = et': 1, 'm+##a = ma': 1, '##a+##g = ##ag': 1, '##g+##n = ##gn': 1, '##n+##a = ##na': 1, 'a+##l = al': 2, '##i+##q = ##iq': 2, '##q+##u = ##qu': 3, '##u+##a = ##ua': 2, 'e+##n = en': 1, '##n+##i = ##ni': 4, '##i+##m = ##im': 3, 'm+##i = mi': 1, 'v+##e = ve': 2, '##e+##n = ##en': 3, '##i+##a = ##ia': 4, '##a+##m = ##am': 2, 'q+##u = qu': 2, '##u+##i = ##ui': 4, 'n+##o = no': 2, '##o+##s = ##os': 1, '##s+##t = ##st': 2, '##t+##r = ##tr': 1, '##r+##u = ##ru': 4, '##u+##d = ##ud': 1, 'e+##x = ex': 3, '##x+##e = ##xe': 1, '##e+##r = ##er': 3, '##r+##c = ##rc': 1, '##t+##a = ##ta': 3, '##a+##t = ##at': 8, '##t+##i = ##ti': 1, '##i+##o = ##io': 1, 'u+##l = ul': 1, '##l+##l = ##ll': 4, '##l+##a = ##la': 2, '##m+##c = ##mc': 1, '##c+##o = ##co': 1, '##r+##i = ##ri': 3, 'n+##i = ni': 1, '##s+##i = ##si': 1, 'e+##a = ea': 1, '##o+##m = ##om': 1, '##m+##m = ##mm': 1, '##d+##o = ##do': 1, '##e+##q = ##eq': 1, 'd+##u = du': 1, 'a+##u = au': 1, '##u+##t = ##ut': 1, 'i+##r = ir': 1, 'r+##e = re': 1, '##e+##p = ##ep': 2, '##p+##r = ##pr': 1, '##e+##h = ##eh': 1, '##h+##e = ##he': 1, '##n+##d = ##nd': 1, '##d+##e = ##de': 2, 'v+##o = vo': 1, '##l+##u = ##lu': 2, '##u+##p = ##up': 2, '##p+##t = ##pt': 2, '##e+##l = ##el': 1, 'e+##s = es': 2, '##s+##s = ##ss': 1, 'c+##i = ci': 1, '##i+##l = ##il': 1, 'e+##u = eu': 1, 'f+##u = fu': 1, '##u+##g = ##ug': 1, '##g+##i = ##gi': 1, 'n+##u = nu': 1, '##u+##l = ##ul': 2, 'p+##a = pa': 1, '##a+##r = ##ar': 1, '##x+##c = ##xc': 1, '##c+##e = ##ce': 1, '##e+##u = ##eu': 1, 'o+##c = oc': 1, '##c+##c = ##cc': 1, '##c+##a = ##ca': 2, '##a+##e = ##ae': 1, 'c+##u = cu': 2, '##d+##a = ##da': 1, 'p+##r = pr': 1, '##r+##o = ##ro': 1, '##o+##i = ##oi': 1, 's+##u = su': 1, '##l+##p = ##lp': 1, '##p+##a = ##pa': 1, 'o+##f = of': 1, '##f+##f = ##ff': 1, '##f+##i = ##fi': 1, '##i+##c = ##ic': 1, 'd+##e = de': 1, '##e+##s = ##es': 1, 'm+##o = mo': 1, 'a+##n = an': 1, 'i+##d = id': 1}\n"
     ]
    }
   ],
   "source": [
    "# second order frequency map\n",
    "# ie frequency of bigrams of tokens in the corpus\n",
    "SF = defaultdict(int)\n",
    "for W in tuples : # word \n",
    "    for B in W : # bigrams in word\n",
    "        SF[generate_merge_token(*B)] += 1\n",
    "print(\"We are looking for which contiguous tokens we can merge\")\n",
    "print(\"frequency of merges : \", dict(SF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams sorted by descending frequency\n",
      "[('##o+##r = ##or', 9), ('##a+##t = ##at', 8), ('##r+##e = ##re', 6), ('##o+##l = ##ol', 6), ('##i+##t = ##it', 6), ('d+##o = do', 5), ('##i+##s = ##is', 5), ('##l+##i = ##li', 5), ('##n+##t = ##nt', 5), ('##l+##o = ##lo', 4), ('##o+##n = ##on', 4), ('##s+##e = ##se', 4), ('##t+##e = ##te', 4), ('##u+##r = ##ur', 4), ('##c+##i = ##ci', 4), ('i+##n = in', 4), ('##i+##d = ##id', 4), ('##n+##i = ##ni', 4), ('##i+##a = ##ia', 4), ('##u+##i = ##ui', 4), ('##r+##u = ##ru', 4), ('##l+##l = ##ll', 4), ('##u+##m = ##um', 3), ('c+##o = co', 3), ('##i+##n = ##in', 3), ('##u+##n = ##un', 3), ('u+##t = ut', 3), ('l+##a = la', 3), ('##a+##b = ##ab', 3), ('##b+##o = ##bo', 3), ('##q+##u = ##qu', 3), ('##i+##m = ##im', 3), ('##e+##n = ##en', 3), ('e+##x = ex', 3), ('##e+##r = ##er', 3), ('##t+##a = ##ta', 3), ('##r+##i = ##ri', 3), ('##e+##m = ##em', 2), ('s+##i = si', 2), ('##e+##t = ##et', 2), ('##n+##s = ##ns', 2), ('##e+##c = ##ec', 2), ('##t+##u = ##tu', 2), ('a+##d = ad', 2), ('##d+##i = ##di', 2), ('##i+##p = ##ip', 2), ('##p+##i = ##pi', 2), ('##m+##o = ##mo', 2), ('##o+##d = ##od', 2), ('a+##l = al', 2), ('##i+##q = ##iq', 2), ('##u+##a = ##ua', 2), ('v+##e = ve', 2), ('##a+##m = ##am', 2), ('q+##u = qu', 2), ('n+##o = no', 2), ('##s+##t = ##st', 2), ('##l+##a = ##la', 2), ('##e+##p = ##ep', 2), ('##d+##e = ##de', 2), ('##l+##u = ##lu', 2), ('##u+##p = ##up', 2), ('##p+##t = ##pt', 2), ('e+##s = es', 2), ('##u+##l = ##ul', 2), ('##c+##a = ##ca', 2), ('c+##u = cu', 2), ('l+##o = lo', 1), ('i+##p = ip', 1), ('##p+##s = ##ps', 1), ('##s+##u = ##su', 1), ('a+##m = am', 1), ('##m+##e = ##me', 1), ('##c+##t = ##ct', 1), ('##s+##c = ##sc', 1), ('##n+##g = ##ng', 1), ('e+##l = el', 1), ('s+##e = se', 1), ('##e+##d = ##ed', 1), ('e+##i = ei', 1), ('##i+##u = ##iu', 1), ('##u+##s = ##us', 1), ('##s+##m = ##sm', 1), ('t+##e = te', 1), ('##m+##p = ##mp', 1), ('##p+##o = ##po', 1), ('##n+##c = ##nc', 1), ('##d+##u = ##du', 1), ('e+##t = et', 1), ('m+##a = ma', 1), ('##a+##g = ##ag', 1), ('##g+##n = ##gn', 1), ('##n+##a = ##na', 1), ('e+##n = en', 1), ('m+##i = mi', 1), ('##o+##s = ##os', 1), ('##t+##r = ##tr', 1), ('##u+##d = ##ud', 1), ('##x+##e = ##xe', 1), ('##r+##c = ##rc', 1), ('##t+##i = ##ti', 1), ('##i+##o = ##io', 1), ('u+##l = ul', 1), ('##m+##c = ##mc', 1), ('##c+##o = ##co', 1), ('n+##i = ni', 1), ('##s+##i = ##si', 1), ('e+##a = ea', 1), ('##o+##m = ##om', 1), ('##m+##m = ##mm', 1), ('##d+##o = ##do', 1), ('##e+##q = ##eq', 1), ('d+##u = du', 1), ('a+##u = au', 1), ('##u+##t = ##ut', 1), ('i+##r = ir', 1), ('r+##e = re', 1), ('##p+##r = ##pr', 1), ('##e+##h = ##eh', 1), ('##h+##e = ##he', 1), ('##n+##d = ##nd', 1), ('v+##o = vo', 1), ('##e+##l = ##el', 1), ('##s+##s = ##ss', 1), ('c+##i = ci', 1), ('##i+##l = ##il', 1), ('e+##u = eu', 1), ('f+##u = fu', 1), ('##u+##g = ##ug', 1), ('##g+##i = ##gi', 1), ('n+##u = nu', 1), ('p+##a = pa', 1), ('##a+##r = ##ar', 1), ('##x+##c = ##xc', 1), ('##c+##e = ##ce', 1), ('##e+##u = ##eu', 1), ('o+##c = oc', 1), ('##c+##c = ##cc', 1), ('##a+##e = ##ae', 1), ('##d+##a = ##da', 1), ('p+##r = pr', 1), ('##r+##o = ##ro', 1), ('##o+##i = ##oi', 1), ('s+##u = su', 1), ('##l+##p = ##lp', 1), ('##p+##a = ##pa', 1), ('o+##f = of', 1), ('##f+##f = ##ff', 1), ('##f+##i = ##fi', 1), ('##i+##c = ##ic', 1), ('d+##e = de', 1), ('##e+##s = ##es', 1), ('m+##o = mo', 1), ('a+##n = an', 1), ('i+##d = id', 1)]\n"
     ]
    }
   ],
   "source": [
    "bi = [(merg,freq) for merg,freq in SF.items()]\n",
    "bi = sorted(bi , key=lambda x:-x[1])\n",
    "print(\"bigrams sorted by descending frequency\")\n",
    "print(bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the token that BPE would choose next\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('##o+##r = ##or', 9)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_merge_byte_pair_encoding = bi[0]\n",
    "print(\"This is the token that BPE would choose next\" )\n",
    "top_merge_byte_pair_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for wordpiece we have to score each AB with the underlying A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens in the corpus :  369\n"
     ]
    }
   ],
   "source": [
    "total_counts = sum(F.values())\n",
    "print(\"total number of tokens in the corpus : \", total_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_b_from_ab(code: str) -> Tuple[str, str]:\n",
    "    return code.split(\"+\")[0], code.split(\"+\")[1].split(\" = \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wp_score(merge_token:str, merge_count:int, F:dict) -> float:\n",
    "    total_counts = sum(F.values())\n",
    "    a,b = get_a_b_from_ab(merge_token)\n",
    "    fa, fb = F.get(a)/total_counts, F.get(b)/total_counts\n",
    "    fab = merge_count/total_counts\n",
    "    return fab / (fa * fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.857142857142858"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_token, merge_count = top_merge_byte_pair_encoding\n",
    "compute_wp_score(merge_token, merge_count, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_scores = [compute_wp_score(x[0],x[1],F) for x in bi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('##o+##r = ##or', 9), 5.857142857142858),\n",
       " (('##a+##t = ##at', 8), 4.328445747800586),\n",
       " (('##r+##e = ##re', 6), 3.9047619047619055),\n",
       " (('##o+##l = ##ol', 6), 4.555555555555556),\n",
       " (('##i+##t = ##it', 6), 2.0405529953917054)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token merging value score\n",
    "bpe_versus_wordpiece_scores = list(zip(bi,wordpiece_scores))\n",
    "bpe_versus_wordpiece_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('o+##f = of', 1), 92.25),\n",
       " (('##f+##f = ##ff', 1), 92.25),\n",
       " (('e+##x = ex', 3), 33.54545454545455),\n",
       " (('o+##c = oc', 1), 18.45)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_preference_list = sorted(bpe_versus_wordpiece_scores, key=lambda x:-x[-1])\n",
    "wordpiece_preference_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE selected the next token : ('##o+##r = ##or', 9)\n",
      "wordpiece selected the next token : (('o+##f = of', 1), 92.25)\n"
     ]
    }
   ],
   "source": [
    "print(f\"BPE selected the next token : {top_merge_byte_pair_encoding}\")\n",
    "print(f\"wordpiece selected the next token : {wordpiece_preference_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next we can use a trie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
