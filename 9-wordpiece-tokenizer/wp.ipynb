{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of WordPiece is that we will consider the following ratio to be the guide for the next tokens to merge : \n",
    "\n",
    "$$R = \\frac{f(AB)}{f(A)f(B)}$$\n",
    "\n",
    "where $f$  is the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = \"lorem ipsum dolor sit amet consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = training_corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_word = lambda word: [word[0]] + [\"##\" + word[i] for i in range(1, len(word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lorem'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is how we build the original vocaubulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['l', '##o', '##r', '##e', '##m']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"this is how we build the original vocaubulary\")\n",
    "explode_word(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(training_corpus: str) -> dict:\n",
    "    words = training_corpus.split()\n",
    "    return [explode_word(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['l', '##o', '##r', '##e', '##m'], ['i', '##p', '##s', '##u', '##m']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = encode(training_corpus)\n",
    "E[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##q',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##x',\n",
       " 'a',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set().union(*[set(word) for word in E])\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need $f$ frequency for each individual token, so that we can compute the denominator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(L:list) -> list:\n",
    "    return [list(zip(sub, sub[1:])) for sub in L]\n",
    "tuples = make_bigrams(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('l', '##o'), ('##o', '##r'), ('##r', '##e'), ('##e', '##m')],\n",
       " [('i', '##p'), ('##p', '##s'), ('##s', '##u'), ('##u', '##m')]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples = make_bigrams(E)\n",
    "tuples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##a+b = ##ab\n"
     ]
    }
   ],
   "source": [
    "def generate_merge_token(p,s):\n",
    "    if p.startswith(\"##\"):\n",
    "        return p + \"+\" + s + \" = ##\"+ (p + s).replace(\"#\",\"\")\n",
    "    return p + \"+\" + s + \" = \" + p + s.replace(\"#\",\"\")\n",
    "\n",
    "print(generate_merge_token(\"##a\",\"b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are looking for which contiguous tokens we can merge\n",
      "frequency of merges :  {'l+##o = lo': 1, '##o+##r = ##or': 9, '##r+##e = ##re': 6, '##e+##m = ##em': 2, 'i+##p = ip': 1, '##p+##s = ##ps': 1, '##s+##u = ##su': 1, '##u+##m = ##um': 3, 'd+##o = do': 5, '##o+##l = ##ol': 6, '##l+##o = ##lo': 4, 's+##i = si': 2, '##i+##t = ##it': 6, 'a+##m = am': 1, '##m+##e = ##me': 1, '##e+##t = ##et': 2, 'c+##o = co': 3, '##o+##n = ##on': 4, '##n+##s = ##ns': 2, '##s+##e = ##se': 4, '##e+##c = ##ec': 2, '##c+##t = ##ct': 1, '##t+##e = ##te': 4, '##t+##u = ##tu': 2, '##u+##r = ##ur': 4, 'a+##d = ad': 2, '##d+##i = ##di': 2, '##i+##p = ##ip': 2, '##p+##i = ##pi': 2, '##i+##s = ##is': 5, '##s+##c = ##sc': 1, '##c+##i = ##ci': 4, '##i+##n = ##in': 3, '##n+##g = ##ng': 1, 'e+##l = el': 1, '##l+##i = ##li': 5, 's+##e = se': 1, '##e+##d = ##ed': 1, 'e+##i = ei': 1, '##i+##u = ##iu': 1, '##u+##s = ##us': 1, '##s+##m = ##sm': 1, '##m+##o = ##mo': 2, '##o+##d = ##od': 2, 't+##e = te': 1, '##m+##p = ##mp': 1, '##p+##o = ##po': 1, 'i+##n = in': 4, '##n+##c = ##nc': 1, '##i+##d = ##id': 4, '##d+##u = ##du': 1, '##u+##n = ##un': 3, '##n+##t = ##nt': 5, 'u+##t = ut': 3, 'l+##a = la': 3, '##a+##b = ##ab': 3, '##b+##o = ##bo': 3, 'e+##t = et': 1, 'm+##a = ma': 1, '##a+##g = ##ag': 1, '##g+##n = ##gn': 1, '##n+##a = ##na': 1, 'a+##l = al': 2, '##i+##q = ##iq': 2, '##q+##u = ##qu': 3, '##u+##a = ##ua': 2, 'e+##n = en': 1, '##n+##i = ##ni': 4, '##i+##m = ##im': 3, 'm+##i = mi': 1, 'v+##e = ve': 2, '##e+##n = ##en': 3, '##i+##a = ##ia': 4, '##a+##m = ##am': 2, 'q+##u = qu': 2, '##u+##i = ##ui': 4, 'n+##o = no': 2, '##o+##s = ##os': 1, '##s+##t = ##st': 2, '##t+##r = ##tr': 1, '##r+##u = ##ru': 4, '##u+##d = ##ud': 1, 'e+##x = ex': 3, '##x+##e = ##xe': 1, '##e+##r = ##er': 3, '##r+##c = ##rc': 1, '##t+##a = ##ta': 3, '##a+##t = ##at': 8, '##t+##i = ##ti': 1, '##i+##o = ##io': 1, 'u+##l = ul': 1, '##l+##l = ##ll': 4, '##l+##a = ##la': 2, '##m+##c = ##mc': 1, '##c+##o = ##co': 1, '##r+##i = ##ri': 3, 'n+##i = ni': 1, '##s+##i = ##si': 1, 'e+##a = ea': 1, '##o+##m = ##om': 1, '##m+##m = ##mm': 1, '##d+##o = ##do': 1, '##e+##q = ##eq': 1, 'd+##u = du': 1, 'a+##u = au': 1, '##u+##t = ##ut': 1, 'i+##r = ir': 1, 'r+##e = re': 1, '##e+##p = ##ep': 2, '##p+##r = ##pr': 1, '##e+##h = ##eh': 1, '##h+##e = ##he': 1, '##n+##d = ##nd': 1, '##d+##e = ##de': 2, 'v+##o = vo': 1, '##l+##u = ##lu': 2, '##u+##p = ##up': 2, '##p+##t = ##pt': 2, '##e+##l = ##el': 1, 'e+##s = es': 2, '##s+##s = ##ss': 1, 'c+##i = ci': 1, '##i+##l = ##il': 1, 'e+##u = eu': 1, 'f+##u = fu': 1, '##u+##g = ##ug': 1, '##g+##i = ##gi': 1, 'n+##u = nu': 1, '##u+##l = ##ul': 2, 'p+##a = pa': 1, '##a+##r = ##ar': 1, '##x+##c = ##xc': 1, '##c+##e = ##ce': 1, '##e+##u = ##eu': 1, 'o+##c = oc': 1, '##c+##c = ##cc': 1, '##c+##a = ##ca': 2, '##a+##e = ##ae': 1, 'c+##u = cu': 2, '##d+##a = ##da': 1, 'p+##r = pr': 1, '##r+##o = ##ro': 1, '##o+##i = ##oi': 1, 's+##u = su': 1, '##l+##p = ##lp': 1, '##p+##a = ##pa': 1, 'o+##f = of': 1, '##f+##f = ##ff': 1, '##f+##i = ##fi': 1, '##i+##c = ##ic': 1, 'd+##e = de': 1, '##e+##s = ##es': 1, 'm+##o = mo': 1, 'a+##n = an': 1, 'i+##d = id': 1}\n"
     ]
    }
   ],
   "source": [
    "# second order frequency map\n",
    "# ie frequency of bigrams of tokens in the corpus\n",
    "SF = defaultdict(int)\n",
    "for W in tuples : # word \n",
    "    for B in W : # bigrams in word\n",
    "        SF[generate_merge_token(*B)] += 1\n",
    "print(\"We are looking for which contiguous tokens we can merge\")\n",
    "print(\"frequency of merges : \", dict(SF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams sorted by descending frequency\n",
      "[('##o+##r = ##or', 9), ('##a+##t = ##at', 8), ('##r+##e = ##re', 6), ('##o+##l = ##ol', 6), ('##i+##t = ##it', 6), ('d+##o = do', 5), ('##i+##s = ##is', 5), ('##l+##i = ##li', 5), ('##n+##t = ##nt', 5), ('##l+##o = ##lo', 4), ('##o+##n = ##on', 4), ('##s+##e = ##se', 4), ('##t+##e = ##te', 4), ('##u+##r = ##ur', 4), ('##c+##i = ##ci', 4), ('i+##n = in', 4), ('##i+##d = ##id', 4), ('##n+##i = ##ni', 4), ('##i+##a = ##ia', 4), ('##u+##i = ##ui', 4), ('##r+##u = ##ru', 4), ('##l+##l = ##ll', 4), ('##u+##m = ##um', 3), ('c+##o = co', 3), ('##i+##n = ##in', 3), ('##u+##n = ##un', 3), ('u+##t = ut', 3), ('l+##a = la', 3), ('##a+##b = ##ab', 3), ('##b+##o = ##bo', 3), ('##q+##u = ##qu', 3), ('##i+##m = ##im', 3), ('##e+##n = ##en', 3), ('e+##x = ex', 3), ('##e+##r = ##er', 3), ('##t+##a = ##ta', 3), ('##r+##i = ##ri', 3), ('##e+##m = ##em', 2), ('s+##i = si', 2), ('##e+##t = ##et', 2), ('##n+##s = ##ns', 2), ('##e+##c = ##ec', 2), ('##t+##u = ##tu', 2), ('a+##d = ad', 2), ('##d+##i = ##di', 2), ('##i+##p = ##ip', 2), ('##p+##i = ##pi', 2), ('##m+##o = ##mo', 2), ('##o+##d = ##od', 2), ('a+##l = al', 2), ('##i+##q = ##iq', 2), ('##u+##a = ##ua', 2), ('v+##e = ve', 2), ('##a+##m = ##am', 2), ('q+##u = qu', 2), ('n+##o = no', 2), ('##s+##t = ##st', 2), ('##l+##a = ##la', 2), ('##e+##p = ##ep', 2), ('##d+##e = ##de', 2), ('##l+##u = ##lu', 2), ('##u+##p = ##up', 2), ('##p+##t = ##pt', 2), ('e+##s = es', 2), ('##u+##l = ##ul', 2), ('##c+##a = ##ca', 2), ('c+##u = cu', 2), ('l+##o = lo', 1), ('i+##p = ip', 1), ('##p+##s = ##ps', 1), ('##s+##u = ##su', 1), ('a+##m = am', 1), ('##m+##e = ##me', 1), ('##c+##t = ##ct', 1), ('##s+##c = ##sc', 1), ('##n+##g = ##ng', 1), ('e+##l = el', 1), ('s+##e = se', 1), ('##e+##d = ##ed', 1), ('e+##i = ei', 1), ('##i+##u = ##iu', 1), ('##u+##s = ##us', 1), ('##s+##m = ##sm', 1), ('t+##e = te', 1), ('##m+##p = ##mp', 1), ('##p+##o = ##po', 1), ('##n+##c = ##nc', 1), ('##d+##u = ##du', 1), ('e+##t = et', 1), ('m+##a = ma', 1), ('##a+##g = ##ag', 1), ('##g+##n = ##gn', 1), ('##n+##a = ##na', 1), ('e+##n = en', 1), ('m+##i = mi', 1), ('##o+##s = ##os', 1), ('##t+##r = ##tr', 1), ('##u+##d = ##ud', 1), ('##x+##e = ##xe', 1), ('##r+##c = ##rc', 1), ('##t+##i = ##ti', 1), ('##i+##o = ##io', 1), ('u+##l = ul', 1), ('##m+##c = ##mc', 1), ('##c+##o = ##co', 1), ('n+##i = ni', 1), ('##s+##i = ##si', 1), ('e+##a = ea', 1), ('##o+##m = ##om', 1), ('##m+##m = ##mm', 1), ('##d+##o = ##do', 1), ('##e+##q = ##eq', 1), ('d+##u = du', 1), ('a+##u = au', 1), ('##u+##t = ##ut', 1), ('i+##r = ir', 1), ('r+##e = re', 1), ('##p+##r = ##pr', 1), ('##e+##h = ##eh', 1), ('##h+##e = ##he', 1), ('##n+##d = ##nd', 1), ('v+##o = vo', 1), ('##e+##l = ##el', 1), ('##s+##s = ##ss', 1), ('c+##i = ci', 1), ('##i+##l = ##il', 1), ('e+##u = eu', 1), ('f+##u = fu', 1), ('##u+##g = ##ug', 1), ('##g+##i = ##gi', 1), ('n+##u = nu', 1), ('p+##a = pa', 1), ('##a+##r = ##ar', 1), ('##x+##c = ##xc', 1), ('##c+##e = ##ce', 1), ('##e+##u = ##eu', 1), ('o+##c = oc', 1), ('##c+##c = ##cc', 1), ('##a+##e = ##ae', 1), ('##d+##a = ##da', 1), ('p+##r = pr', 1), ('##r+##o = ##ro', 1), ('##o+##i = ##oi', 1), ('s+##u = su', 1), ('##l+##p = ##lp', 1), ('##p+##a = ##pa', 1), ('o+##f = of', 1), ('##f+##f = ##ff', 1), ('##f+##i = ##fi', 1), ('##i+##c = ##ic', 1), ('d+##e = de', 1), ('##e+##s = ##es', 1), ('m+##o = mo', 1), ('a+##n = an', 1), ('i+##d = id', 1)]\n"
     ]
    }
   ],
   "source": [
    "bi = [(merg,freq) for merg,freq in SF.items()]\n",
    "bi = sorted(bi , key=lambda x:-x[1])\n",
    "print(\"bigrams sorted by descending frequency\")\n",
    "print(bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the token that BPE would choose next\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('##o+##r = ##or', 9)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_merge_byte_pair_encoding = bi[0]\n",
    "print(\"This is the token that BPE would choose next\" )\n",
    "top_merge_byte_pair_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for wordpiece we have to score each AB with the underlying A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('##o', '##r')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_a_b_from_ab(code:str) -> tuple:\n",
    "    return code.split(\"+\")[0] , code.split(\"+\")[1].split(\" = \")[0]\n",
    "\n",
    "get_a_b_from_ab(top_merge_byte_pair_encoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m F\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_a_b_from_ab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_a_b_from_ab_operands \u001b[38;5;241m=\u001b[39m get_a_b_from_ab\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_a_b_from_ab' is not defined"
     ]
    }
   ],
   "source": [
    "get_a_b_from_ab_operands = get_a_b_from_ab.__code__.co_varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.new_representation(pre, suff)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1255235206.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[79], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    apply_merge(encoded_corpus, top_merge):\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "apply_merge(encoded_corpus, top_merge):\n",
    "    return [new_representation(*T) for T in encoded_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['l', '##o', '##r', '##e', '##m'],\n",
       " ['i', '##p', '##s', '##u', '##m'],\n",
       " ['d', '##o', '##l', '##o', '##r'],\n",
       " ['s', '##i', '##t'],\n",
       " ['a', '##m', '##e', '##t'],\n",
       " ['c', '##o', '##n', '##s', '##e', '##c', '##t', '##e', '##t', '##u', '##r'],\n",
       " ['a', '##d', '##i', '##p', '##i', '##s', '##c', '##i', '##n', '##g'],\n",
       " ['e', '##l', '##i', '##t'],\n",
       " ['s', '##e', '##d'],\n",
       " ['d', '##o'],\n",
       " ['e', '##i', '##u', '##s', '##m', '##o', '##d'],\n",
       " ['t', '##e', '##m', '##p', '##o', '##r'],\n",
       " ['i', '##n', '##c', '##i', '##d', '##i', '##d', '##u', '##n', '##t'],\n",
       " ['u', '##t'],\n",
       " ['l', '##a', '##b', '##o', '##r', '##e'],\n",
       " ['e', '##t'],\n",
       " ['d', '##o', '##l', '##o', '##r', '##e'],\n",
       " ['m', '##a', '##g', '##n', '##a'],\n",
       " ['a', '##l', '##i', '##q', '##u', '##a'],\n",
       " ['u', '##t'],\n",
       " ['e', '##n', '##i', '##m'],\n",
       " ['a', '##d'],\n",
       " ['m', '##i', '##n', '##i', '##m'],\n",
       " ['v', '##e', '##n', '##i', '##a', '##m'],\n",
       " ['q', '##u', '##i', '##s'],\n",
       " ['n', '##o', '##s', '##t', '##r', '##u', '##d'],\n",
       " ['e',\n",
       "  '##x',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##c',\n",
       "  '##i',\n",
       "  '##t',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " ['u', '##l', '##l', '##a', '##m', '##c', '##o'],\n",
       " ['l', '##a', '##b', '##o', '##r', '##i', '##s'],\n",
       " ['n', '##i', '##s', '##i'],\n",
       " ['u', '##t'],\n",
       " ['a', '##l', '##i', '##q', '##u', '##i', '##p'],\n",
       " ['e', '##x'],\n",
       " ['e', '##a'],\n",
       " ['c', '##o', '##m', '##m', '##o', '##d', '##o'],\n",
       " ['c', '##o', '##n', '##s', '##e', '##q', '##u', '##a', '##t'],\n",
       " ['d', '##u', '##i', '##s'],\n",
       " ['a', '##u', '##t', '##e'],\n",
       " ['i', '##r', '##u', '##r', '##e'],\n",
       " ['d', '##o', '##l', '##o', '##r'],\n",
       " ['i', '##n'],\n",
       " ['r',\n",
       "  '##e',\n",
       "  '##p',\n",
       "  '##r',\n",
       "  '##e',\n",
       "  '##h',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##d',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##i',\n",
       "  '##t'],\n",
       " ['i', '##n'],\n",
       " ['v', '##o', '##l', '##u', '##p', '##t', '##a', '##t', '##e'],\n",
       " ['v', '##e', '##l', '##i', '##t'],\n",
       " ['e', '##s', '##s', '##e'],\n",
       " ['c', '##i', '##l', '##l', '##u', '##m'],\n",
       " ['d', '##o', '##l', '##o', '##r', '##e'],\n",
       " ['e', '##u'],\n",
       " ['f', '##u', '##g', '##i', '##a', '##t'],\n",
       " ['n', '##u', '##l', '##l', '##a'],\n",
       " ['p', '##a', '##r', '##i', '##a', '##t', '##u', '##r'],\n",
       " ['e', '##x', '##c', '##e', '##p', '##t', '##e', '##u', '##r'],\n",
       " ['s', '##i', '##n', '##t'],\n",
       " ['o', '##c', '##c', '##a', '##e', '##c', '##a', '##t'],\n",
       " ['c', '##u', '##p', '##i', '##d', '##a', '##t', '##a', '##t'],\n",
       " ['n', '##o', '##n'],\n",
       " ['p', '##r', '##o', '##i', '##d', '##e', '##n', '##t'],\n",
       " ['s', '##u', '##n', '##t'],\n",
       " ['i', '##n'],\n",
       " ['c', '##u', '##l', '##p', '##a'],\n",
       " ['q', '##u', '##i'],\n",
       " ['o', '##f', '##f', '##i', '##c', '##i', '##a'],\n",
       " ['d', '##e', '##s', '##e', '##r', '##u', '##n', '##t'],\n",
       " ['m', '##o', '##l', '##l', '##i', '##t'],\n",
       " ['a', '##n', '##i', '##m'],\n",
       " ['i', '##d'],\n",
       " ['e', '##s', '##t'],\n",
       " ['l', '##a', '##b', '##o', '##r', '##u', '##m']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be faster we have to use a trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m potential_merges \u001b[38;5;241m=\u001b[39m [ new_representation(p,s) \u001b[38;5;28;01mfor\u001b[39;00m (p,s) \u001b[38;5;129;01min\u001b[39;00m L \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m make_tuples(E)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "potential_merges = [ new_representation(p,s) for (p,s) in L for L in make_tuples(E)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<zip at 0x1204ee140>,\n",
       " <zip at 0x1204ee300>,\n",
       " <zip at 0x1204ee880>,\n",
       " <zip at 0x1204ec500>,\n",
       " <zip at 0x1204ec180>,\n",
       " <zip at 0x1204eeb80>,\n",
       " <zip at 0x1204ef740>,\n",
       " <zip at 0x117c8ad00>,\n",
       " <zip at 0x117c88d80>,\n",
       " <zip at 0x1204ec880>,\n",
       " <zip at 0x117c8a640>,\n",
       " <zip at 0x117c8ac40>,\n",
       " <zip at 0x1117a6780>,\n",
       " <zip at 0x1117a6900>,\n",
       " <zip at 0x1117a6480>,\n",
       " <zip at 0x1117a7c40>,\n",
       " <zip at 0x1117a7400>,\n",
       " <zip at 0x1117a7600>,\n",
       " <zip at 0x1117a6540>,\n",
       " <zip at 0x1117a6bc0>,\n",
       " <zip at 0x12054bb80>,\n",
       " <zip at 0x12054a380>,\n",
       " <zip at 0x1204f2d40>,\n",
       " <zip at 0x1204f0e80>,\n",
       " <zip at 0x1204f1240>,\n",
       " <zip at 0x1204f3980>,\n",
       " <zip at 0x1204f1d40>,\n",
       " <zip at 0x1204f0f40>,\n",
       " <zip at 0x1204f3900>,\n",
       " <zip at 0x1204f0200>,\n",
       " <zip at 0x1204f33c0>,\n",
       " <zip at 0x1204f0900>,\n",
       " <zip at 0x1204f2d80>,\n",
       " <zip at 0x1204f1540>,\n",
       " <zip at 0x1204f1480>,\n",
       " <zip at 0x1204f1880>,\n",
       " <zip at 0x1204f3e40>,\n",
       " <zip at 0x1204f3c80>,\n",
       " <zip at 0x1204f1100>,\n",
       " <zip at 0x1204f06c0>,\n",
       " <zip at 0x1204f2140>,\n",
       " <zip at 0x1204f3a40>,\n",
       " <zip at 0x1204f1dc0>,\n",
       " <zip at 0x1204f19c0>,\n",
       " <zip at 0x1204f3600>,\n",
       " <zip at 0x1204f1c80>,\n",
       " <zip at 0x1204f0600>,\n",
       " <zip at 0x1204f3f00>,\n",
       " <zip at 0x1204f3f80>,\n",
       " <zip at 0x1204f2800>,\n",
       " <zip at 0x1204f2d00>,\n",
       " <zip at 0x1204f3040>,\n",
       " <zip at 0x1204f0c00>,\n",
       " <zip at 0x1204f1fc0>,\n",
       " <zip at 0x120486f00>,\n",
       " <zip at 0x120487fc0>,\n",
       " <zip at 0x1204863c0>,\n",
       " <zip at 0x120484140>,\n",
       " <zip at 0x117c849c0>,\n",
       " <zip at 0x1116fe680>,\n",
       " <zip at 0x1116fc180>,\n",
       " <zip at 0x1116fc200>,\n",
       " <zip at 0x111777840>,\n",
       " <zip at 0x1204fc980>,\n",
       " <zip at 0x1204fe1c0>,\n",
       " <zip at 0x1204fd700>,\n",
       " <zip at 0x1204ff040>,\n",
       " <zip at 0x1204fc500>,\n",
       " <zip at 0x1204ff080>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_tuples(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 4,\n",
       " '##o': 27,\n",
       " '##r': 21,\n",
       " '##e': 27,\n",
       " '##m': 14,\n",
       " 'i': 7,\n",
       " '##p': 9,\n",
       " '##s': 14,\n",
       " '##u': 25,\n",
       " 'd': 7,\n",
       " '##l': 18,\n",
       " 's': 4,\n",
       " '##i': 35,\n",
       " '##t': 31,\n",
       " 'a': 7,\n",
       " 'c': 6,\n",
       " '##n': 20,\n",
       " '##c': 10,\n",
       " '##d': 12,\n",
       " '##g': 3,\n",
       " 'e': 11,\n",
       " 't': 1,\n",
       " 'u': 4,\n",
       " '##a': 22,\n",
       " '##b': 3,\n",
       " 'm': 3,\n",
       " '##q': 3,\n",
       " 'v': 3,\n",
       " 'q': 2,\n",
       " 'n': 4,\n",
       " '##x': 3,\n",
       " 'r': 1,\n",
       " '##h': 1,\n",
       " 'f': 1,\n",
       " 'p': 2,\n",
       " 'o': 2,\n",
       " '##f': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freq_map(encoded_training_corpus: list) -> dict:\n",
    "    freq_map = {}\n",
    "    for word in encoded_training_corpus:\n",
    "        for token in word:\n",
    "            if token in freq_map:\n",
    "                freq_map[token] += 1\n",
    "            else:\n",
    "                freq_map[token] = 1\n",
    "    return freq_map\n",
    "\n",
    "NUM_STEPS = 1\n",
    "\n",
    "E = encode(training_corpus)\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    # first order frequency\n",
    "    F = freq_map(E)\n",
    "    # second order frequency\n",
    "#     F2 = freq_map(encode(\" \".join(encode(training_corpus))))\n",
    "#     for word in training_corpus.split():\n",
    "#         for token in explode_word(word):\n",
    "#             if F[token] < THRESHOLD:\n",
    "#                 training_corpus = training_corpus.replace(word, token)\n",
    "#                 break\n",
    "# F = freq_map(encode(training_corpus))\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_seq, vocabulary):\n",
    "    \"\"\" we will break playing into p, ##l\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
