{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, List\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad import nn\n",
    "from tinygrad.nn.optim import Optimizer\n",
    "\n",
    "# from tinygrad.extra.lr_scheduler import LR_Scheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "DEGREE = 5 # degree of polynomial to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "Let's hack the insolubility of computing the roots of a polynomial of degree 5,\n",
    " by using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x: np.ndarray) -> Callable[[np.ndarray,str,str], None]:\n",
    "    \"\"\"\n",
    "    Curries X into the plot function\n",
    "    \"\"\"\n",
    "    def fn(y: np.ndarray, label: str, color: str) -> None:\n",
    "        plt.plot(x, y, label=label, color=color)\n",
    "    return lambda y,label,color : fn(y,label,color)\n",
    "\n",
    "\n",
    "def plt_setup(xlim:tuple = (0,1), ylim:tuple = (0,1), title:str = \"getting $a*(X^2)+b$ from Relus is the goal\") -> None:\n",
    "    plt.grid(True)\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(title)\n",
    "    plt.style.use('dark_background')\n",
    "    return None\n",
    "\n",
    "color_cycle = matplotlib.colormaps[\"Spectral\"]\n",
    "\n",
    "X=np.arange(0,1,0.001)\n",
    "p = plot(X)\n",
    "\n",
    "# from tinygrad.extra\n",
    "class LR_Scheduler:\n",
    "  def __init__(self, optimizer: Optimizer):\n",
    "    self.optimizer = optimizer\n",
    "    self.epoch_counter = Tensor([0], requires_grad=False, device=self.optimizer.device)\n",
    "\n",
    "  def get_lr(self): pass\n",
    "\n",
    "  def step(self) -> None:\n",
    "    self.epoch_counter.assign(self.epoch_counter + 1).realize()\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize()\n",
    "\n",
    "class OneCycleLR(LR_Scheduler):\n",
    "  def __init__(self, optimizer: Optimizer, max_lr: float, div_factor: float, final_div_factor: float, total_steps: int, pct_start: float):\n",
    "    super().__init__(optimizer)\n",
    "    self.initial_lr = max_lr / div_factor\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = self.initial_lr / final_div_factor\n",
    "    self.total_steps = total_steps\n",
    "    self.pct_start = pct_start\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize() # update the initial LR\n",
    "\n",
    "  @staticmethod\n",
    "  def _annealing_linear(start: float, end: float, pct: Tensor) -> Tensor: return (pct*(end-start)+start)\n",
    "\n",
    "  def get_lr(self) -> Tensor:\n",
    "    return (self.epoch_counter < self.total_steps*self.pct_start).where(\n",
    "      self._annealing_linear(self.initial_lr, self.max_lr, self.epoch_counter/(self.total_steps*self.pct_start)),\n",
    "      self._annealing_linear(self.max_lr, self.min_lr, (self.epoch_counter-(self.total_steps*self.pct_start))/(self.total_steps*(1-self.pct_start)))\n",
    "    )\n",
    "    \n",
    "class Model:\n",
    "  # inputs : a, b, c, d, e (coefficients of polynomial)  -> actually just b,c,d,e because a is always 1\n",
    "  # expected output = roots z1, z2, z3, z4, z5 (assumed to be reals for now)\n",
    "\n",
    "  def __init__(self, layers:int = 3):\n",
    "    self.layers = [nn.Linear(5, 3), Tensor.relu] + [nn.Linear(3, 3), Tensor.relu]*layers + [nn.Linear(3, DEGREE), Tensor.sigmoid]\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor: return x.sequential(self.layers)\n",
    "\n",
    "  def L1(self) -> Tensor: return sum([l.weight.abs().sum() + l.bias.abs().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "  def L2(self) -> Tensor: return sum([l.weight.square().sum() + l.bias.square().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "\n",
    "def train_step(x:Tensor, y:Tensor, model: Model, opt: nn.optim.LAMB, lr_schedule: LR_Scheduler) -> Tensor:\n",
    "    y_pred = model(x)\n",
    "    loss = (y_pred - y).square().mean() #+ 0.0001 * (model.L2())# + 0.0001 * (model.L2())\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    lr_schedule.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def plot_model(model, title:str = \"neural network learning versus the takagi curve\") -> None:\n",
    "    #p(target(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"target\", color=color_cycle(0.0))\n",
    "    p(model(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"model\", color=color_cycle(0.5))\n",
    "    #p(T_1(X)+T_2(X)+T_3(X), \"Takagi 3\", color=color_cycle(0.75))\n",
    "    plt_setup(xlim=(0,1), ylim=(0,1/2+0.1), title=title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "roots = [1,2,3,4,5]\n",
    "print(sum(r1 * r2 for r1, r2 in combinations(roots, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "Tensor.ones((1,5)).numpy()\n",
    "\n",
    "for i in range(5):\n",
    "    print(Tensor.ones((1,5))[:,i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stack in module tinygrad.tensor:\n",
      "\n",
      "stack(tensors: 'Sequence[Tensor]', dim: 'int' = 0) -> 'Tensor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Tensor.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.0011978022521361709\n",
      "loss at train_step = 0 : 33.457515716552734\n",
      "lr = 0.020978020504117012\n",
      "loss at train_step = 100 : 33.3631706237793\n",
      "lr = 0.04075824096798897\n",
      "loss at train_step = 200 : 33.33977508544922\n",
      "lr = 0.060538459569215775\n",
      "loss at train_step = 300 : 33.20367431640625\n",
      "lr = 0.08031868189573288\n",
      "loss at train_step = 400 : 33.43901443481445\n",
      "lr = 0.09990011155605316\n",
      "loss at train_step = 500 : 33.30876922607422\n",
      "lr = 0.07992208749055862\n",
      "loss at train_step = 600 : 33.2637825012207\n",
      "lr = 0.059944067150354385\n",
      "loss at train_step = 700 : 33.391746520996094\n",
      "lr = 0.039966046810150146\n",
      "loss at train_step = 800 : 33.44405746459961\n",
      "lr = 0.01998802460730076\n",
      "loss at train_step = 900 : 33.3244514465332\n",
      "lr = 1.0002695489674807e-05\n",
      "loss at train_step = 1000 : 33.444541931152344\n"
     ]
    }
   ],
   "source": [
    "# we write the quintic polynomial P(z)\n",
    "# (z-z1)(z-z2)(z-z3)(z-z4)(z-z5)\n",
    "\"\"\" \n",
    "Given a polynomial of degree 5: Vieta's formulas.\n",
    "$p(x) = x^5 + a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0$\n",
    "\n",
    "where the roots are r_1, r_2, r_3, r_4, r_5, the coefficients can be calculated as follows:\n",
    "\n",
    "- $a_4 = -(r_1 + r_2 + r_3 + r_4 + r_5)$\n",
    "- $a_3 = r_1r_2 + r_1r_3 + r_1r_4 + r_1r_5 + r_2r_3 + r_2r_4 + r_2r_5 + r_3r_4 + r_3r_5 + r_4r_5$\n",
    "- $a_2 = -(r_1r_2r_3 + r_1r_2r_4 + r_1r_2r_5 + r_1r_3r_4 + r_1r_3r_5 + r_1r_4r_5 + r_2r_3r_4 + r_2r_3r_5 + r_2r_4r_5 + r_3r_4r_5)$\n",
    "- $a_1 = r_1r_2r_3r_4 + r_1r_2r_3r_5 + r_1r_2r_4r_5 + r_1r_3r_4r_5 + r_2r_3r_4r_5$\n",
    "- $a_0 = -r_1r_2r_3r_4r_5$\n",
    "\n",
    "# Example usage\n",
    "coefficients = polynomial_coefficients(1, 2, 3, 4, 5)\n",
    "print(coefficients)\n",
    "\"\"\"\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "#function decaorator that prints arguments and function name\n",
    "def print_args(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        print(f\"called {func.__name__}\\n\"\n",
    "              f\"with args: {args}, kwargs: {kwargs}\")\n",
    "        return_value = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__} returns {return_value}\")\n",
    "        return return_value\n",
    "    return inner\n",
    "\n",
    "\n",
    "# @print_args\n",
    "def polynomial_coefficients(roots: Tensor) -> Tensor:\n",
    "    roots = [roots[:,i] for i in range(5)]\n",
    "    if len(roots) != 5:\n",
    "        raise ValueError(f\"Exactly 5 roots are required for a polynomial of degree 5., got {len(roots)}, got roots = {roots}\")\n",
    "    a4 = -sum(roots)\n",
    "    a3 = sum(r1*2 for r1, r2 in combinations(roots, 2))\n",
    "    a2 = -sum(r1*r2*r3 for r1, r2, r3 in combinations(roots, 3))\n",
    "    a1 = sum(r1*r2*r3*r4 for r1, r2, r3, r4 in combinations(roots, 4))\n",
    "    a0 = -roots[0]*roots[1]*roots[2]*roots[3]*roots[4]\n",
    "    return Tensor.stack([a4, a3, a2, a1, a0],dim=1)\n",
    "\n",
    "\n",
    "# step 1 train a neural net to learn the roots\n",
    "def train_model(model, lr:float = 0.01, steps:int = 1001, bs:int = 32768) -> Model:\n",
    "    opt = nn.optim.Adam(nn.state.get_parameters(model), lr)\n",
    "    lr_schedule = OneCycleLR(opt, max_lr=0.1, div_factor=100, final_div_factor=100, total_steps=steps, pct_start=0.5)\n",
    "    old_lr = opt.lr.numpy()\n",
    "    for i in range(steps):\n",
    "        roots = (20.0*Tensor.rand(bs, 5)-10.0).realize() # sample the roots between -10 and 10\n",
    "        decoded_inputs = polynomial_coefficients(roots) # see if we can call this on a Tensor ? \n",
    "        loss = train_step(x=decoded_inputs, y=roots, model=model, opt=opt, lr_schedule=lr_schedule)\n",
    "        if i%100 == 0:\n",
    "            print(f\"lr = {opt.lr.numpy()[0]}\")\n",
    "            print(f\"loss at train_step = {i} : {loss.numpy()}\")\n",
    "    return model\n",
    "\n",
    "# step 3 apply it to a quintic polynomial\n",
    "\n",
    "model = train_model(Model(layers=25))\n",
    "  \n",
    "# step 4 use newton's method to find the roots given the initialization point given by the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb",
   "language": "python",
   "name": "aifb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
