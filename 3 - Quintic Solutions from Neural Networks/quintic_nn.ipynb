{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad import nn\n",
    "from tinygrad.nn.optim import Optimizer\n",
    "\n",
    "# from tinygrad.extra.lr_scheduler import LR_Scheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x: np.ndarray) -> Callable[[np.ndarray,str,str], None]:\n",
    "    \"\"\"\n",
    "    Curries X into the plot function\n",
    "    \"\"\"\n",
    "    def fn(y: np.ndarray, label: str, color: str) -> None:\n",
    "        plt.plot(x, y, label=label, color=color)\n",
    "    return lambda y,label,color : fn(y,label,color)\n",
    "\n",
    "\n",
    "def plt_setup(xlim:tuple = (0,1), ylim:tuple = (0,1), title:str = \"getting $a*(X^2)+b$ from Relus is the goal\") -> None:\n",
    "    plt.grid(True)\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(title)\n",
    "    plt.style.use('dark_background')\n",
    "    return None\n",
    "\n",
    "color_cycle = matplotlib.colormaps[\"Spectral\"]\n",
    "\n",
    "X=np.arange(0,1,0.001)\n",
    "p = plot(X)\n",
    "\n",
    "# from tinygrad.extra\n",
    "class LR_Scheduler:\n",
    "  def __init__(self, optimizer: Optimizer):\n",
    "    self.optimizer = optimizer\n",
    "    self.epoch_counter = Tensor([0], requires_grad=False, device=self.optimizer.device)\n",
    "\n",
    "  def get_lr(self): pass\n",
    "\n",
    "  def step(self) -> None:\n",
    "    self.epoch_counter.assign(self.epoch_counter + 1).realize()\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize()\n",
    "\n",
    "class OneCycleLR(LR_Scheduler):\n",
    "  def __init__(self, optimizer: Optimizer, max_lr: float, div_factor: float, final_div_factor: float, total_steps: int, pct_start: float):\n",
    "    super().__init__(optimizer)\n",
    "    self.initial_lr = max_lr / div_factor\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = self.initial_lr / final_div_factor\n",
    "    self.total_steps = total_steps\n",
    "    self.pct_start = pct_start\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize() # update the initial LR\n",
    "\n",
    "  @staticmethod\n",
    "  def _annealing_linear(start: float, end: float, pct: Tensor) -> Tensor: return (pct*(end-start)+start)\n",
    "\n",
    "  def get_lr(self) -> Tensor:\n",
    "    return (self.epoch_counter < self.total_steps*self.pct_start).where(\n",
    "      self._annealing_linear(self.initial_lr, self.max_lr, self.epoch_counter/(self.total_steps*self.pct_start)),\n",
    "      self._annealing_linear(self.max_lr, self.min_lr, (self.epoch_counter-(self.total_steps*self.pct_start))/(self.total_steps*(1-self.pct_start)))\n",
    "    )\n",
    "    \n",
    "class Model:\n",
    "  def __init__(self, layers:int = 3):\n",
    "    self.layers = [nn.Linear(1, 3), Tensor.relu] + [nn.Linear(3, 3), Tensor.relu]*layers + [nn.Linear(3, 1)]\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor: return x.sequential(self.layers)\n",
    "\n",
    "  def L1(self) -> Tensor: return sum([l.weight.abs().sum() + l.bias.abs().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "  def L2(self) -> Tensor: return sum([l.weight.square().sum() + l.bias.square().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "\n",
    "def train_step(x:Tensor, y:Tensor, model: Model, opt: nn.optim.LAMB, lr_schedule: LR_Scheduler) -> Tensor:\n",
    "    y_pred = model(x)\n",
    "    loss = (y_pred - y).square().mean() #+ 0.0001 * (model.L2())# + 0.0001 * (model.L2())\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    lr_schedule.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(model, lr:float = 0.01, steps:int = 1001, bs:int = 32768) -> Model:\n",
    "    opt = nn.optim.Adam(nn.state.get_parameters(model), lr)\n",
    "    lr_schedule = OneCycleLR(opt, max_lr=0.1, div_factor=100, final_div_factor=100, total_steps=steps, pct_start=0.5)\n",
    "    old_lr = opt.lr.numpy()\n",
    "    for i in range(steps):\n",
    "        samples = Tensor.rand(bs, 1).realize()\n",
    "        y = target(samples)\n",
    "        loss = train_step(samples, y, model, opt, lr_schedule)\n",
    "        if i%100 == 0:\n",
    "            print(f\"lr = {opt.lr.numpy()[0]}\")\n",
    "            print(f\"loss at train_step = {i} : {loss.numpy()}\")\n",
    "    return model\n",
    "\n",
    "def plot_model(model, title:str = \"neural network learning versus the takagi curve\") -> None:\n",
    "    #p(target(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"target\", color=color_cycle(0.0))\n",
    "    p(model(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"model\", color=color_cycle(0.5))\n",
    "    #p(T_1(X)+T_2(X)+T_3(X), \"Takagi 3\", color=color_cycle(0.75))\n",
    "    plt_setup(xlim=(0,1), ylim=(0,1/2+0.1), title=title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we write the quintic polynomial P(z)\n",
    "# (z-z1)(z-z2)(z-z3)(z-z4)(z-z5)\n",
    "\n",
    "# step 1 sample the roots\n",
    "R = Tensor.rand(5, 1).realize()\n",
    "\n",
    "# step2 train a neural net to learn the roots\n",
    "\n",
    "# step 3 apply it to a quintic polynomial\n",
    "\n",
    "# step 4 use newton's method to find the roots given the initialization point given by the neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb",
   "language": "python",
   "name": "aifb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
