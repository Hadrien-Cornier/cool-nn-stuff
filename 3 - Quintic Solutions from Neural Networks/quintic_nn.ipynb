{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, List\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad import nn\n",
    "from tinygrad.nn.optim import Optimizer\n",
    "\n",
    "# from tinygrad.extra.lr_scheduler import LR_Scheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "DEGREE = 5 # degree of polynomial to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "Let's hack the insolubility of computing the roots of a polynomial of degree 5,\n",
    " by using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x: np.ndarray) -> Callable[[np.ndarray,str,str], None]:\n",
    "    \"\"\"\n",
    "    Curries X into the plot function\n",
    "    \"\"\"\n",
    "    def fn(y: np.ndarray, label: str, color: str) -> None:\n",
    "        plt.plot(x, y, label=label, color=color)\n",
    "    return lambda y,label,color : fn(y,label,color)\n",
    "\n",
    "\n",
    "def plt_setup(xlim:tuple = (0,1), ylim:tuple = (0,1), title:str = \"getting $a*(X^2)+b$ from Relus is the goal\") -> None:\n",
    "    plt.grid(True)\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(title)\n",
    "    plt.style.use('dark_background')\n",
    "    return None\n",
    "\n",
    "color_cycle = matplotlib.colormaps[\"Spectral\"]\n",
    "\n",
    "X=np.arange(0,1,0.001)\n",
    "p = plot(X)\n",
    "\n",
    "# from tinygrad.extra\n",
    "class LR_Scheduler:\n",
    "  def __init__(self, optimizer: Optimizer):\n",
    "    self.optimizer = optimizer\n",
    "    self.epoch_counter = Tensor([0], requires_grad=False, device=self.optimizer.device)\n",
    "\n",
    "  def get_lr(self): pass\n",
    "\n",
    "  def step(self) -> None:\n",
    "    self.epoch_counter.assign(self.epoch_counter + 1).realize()\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize()\n",
    "\n",
    "class OneCycleLR(LR_Scheduler):\n",
    "  def __init__(self, optimizer: Optimizer, max_lr: float, div_factor: float, final_div_factor: float, total_steps: int, pct_start: float):\n",
    "    super().__init__(optimizer)\n",
    "    self.initial_lr = max_lr / div_factor\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = self.initial_lr / final_div_factor\n",
    "    self.total_steps = total_steps\n",
    "    self.pct_start = pct_start\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize() # update the initial LR\n",
    "\n",
    "  @staticmethod\n",
    "  def _annealing_linear(start: float, end: float, pct: Tensor) -> Tensor: return (pct*(end-start)+start)\n",
    "\n",
    "  def get_lr(self) -> Tensor:\n",
    "    return (self.epoch_counter < self.total_steps*self.pct_start).where(\n",
    "      self._annealing_linear(self.initial_lr, self.max_lr, self.epoch_counter/(self.total_steps*self.pct_start)),\n",
    "      self._annealing_linear(self.max_lr, self.min_lr, (self.epoch_counter-(self.total_steps*self.pct_start))/(self.total_steps*(1-self.pct_start)))\n",
    "    )\n",
    "    \n",
    "class Model:\n",
    "  # inputs : a, b, c, d, e (coefficients of polynomial)  -> actually just b,c,d,e because a is always 1\n",
    "  # expected output = roots z1, z2, z3, z4, z5 (assumed to be reals for now)\n",
    "\n",
    "  def __init__(self, layers:int = 3):\n",
    "    self.layers = [nn.Linear(4, 3), Tensor.relu] + [nn.Linear(3, 3), Tensor.relu]*layers + [nn.Linear(3, DEGREE), Tensor.sigmoid]\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor: return x.sequential(self.layers)\n",
    "\n",
    "  def L1(self) -> Tensor: return sum([l.weight.abs().sum() + l.bias.abs().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "  def L2(self) -> Tensor: return sum([l.weight.square().sum() + l.bias.square().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "\n",
    "def train_step(x:Tensor, y:Tensor, model: Model, opt: nn.optim.LAMB, lr_schedule: LR_Scheduler) -> Tensor:\n",
    "    y_pred = model(x)\n",
    "    loss = (y_pred - y).square().mean() #+ 0.0001 * (model.L2())# + 0.0001 * (model.L2())\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    lr_schedule.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def plot_model(model, title:str = \"neural network learning versus the takagi curve\") -> None:\n",
    "    #p(target(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"target\", color=color_cycle(0.0))\n",
    "    p(model(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"model\", color=color_cycle(0.5))\n",
    "    #p(T_1(X)+T_2(X)+T_3(X), \"Takagi 3\", color=color_cycle(0.75))\n",
    "    plt_setup(xlim=(0,1), ylim=(0,1/2+0.1), title=title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "roots = [1,2,3,4,5]\n",
    "print(sum(r1 * r2 for r1, r2 in combinations(roots, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "Tensor.ones((1,5)).numpy()\n",
    "\n",
    "for i in range(5):\n",
    "    print(Tensor.ones((1,5))[:,i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stack in module tinygrad.tensor:\n",
      "\n",
      "stack(tensors: 'Sequence[Tensor]', dim: 'int' = 0) -> 'Tensor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Tensor.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called polynomial_coefficients\n",
      "with args: (<Tensor <LB METAL (32768, 5) contig:True (<LoadOps.CUSTOM: 5>, <buf device:METAL size:163840 dtype:dtypes.float>)> on METAL with grad None>,), kwargs: {}\n",
      "polynomial_coefficients returns <Tensor <LB METAL (32768, 5) contig:True (<BinaryOps.ADD: 1>, None)> on METAL with grad None>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input Tensor shapes (32768, 5) and (4, 3) cannot be multiplied (5 != 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# step 3 apply it to a quintic polynomial\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# step 4 use newton's method to find the roots given the initialization point given by the neural network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, lr, steps, bs)\u001b[0m\n\u001b[1;32m     53\u001b[0m roots \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mrand(bs, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mrealize() \u001b[38;5;66;03m# sample the roots\u001b[39;00m\n\u001b[1;32m     54\u001b[0m decoded_inputs \u001b[38;5;241m=\u001b[39m polynomial_coefficients(roots) \u001b[38;5;66;03m# see if we can call this on a Tensor ? \u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 72\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, y, model, opt, lr_schedule)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(x:Tensor, y:Tensor, model: Model, opt: nn\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mLAMB, lr_schedule: LR_Scheduler) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 72\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m-\u001b[39m y)\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m#+ 0.0001 * (model.L2())# + 0.0001 * (model.L2())\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[43], line 64\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequential\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aiforbattle/venv/lib/python3.9/site-packages/tinygrad/tensor.py:839\u001b[0m, in \u001b[0;36mTensor.sequential\u001b[0;34m(self, ll)\u001b[0m\n\u001b[0;32m--> 839\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msequential\u001b[39m(\u001b[38;5;28mself\u001b[39m, ll:List[Callable[[Tensor], Tensor]]): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aiforbattle/venv/lib/python3.9/site-packages/tinygrad/tensor.py:839\u001b[0m, in \u001b[0;36mTensor.sequential.<locals>.<lambda>\u001b[0;34m(x, f)\u001b[0m\n\u001b[0;32m--> 839\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msequential\u001b[39m(\u001b[38;5;28mself\u001b[39m, ll:List[Callable[[Tensor], Tensor]]): \u001b[38;5;28;01mreturn\u001b[39;00m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x,f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, ll, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/aiforbattle/venv/lib/python3.9/site-packages/tinygrad/nn/__init__.py:81\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:Tensor):\n\u001b[0;32m---> 81\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aiforbattle/venv/lib/python3.9/site-packages/tinygrad/tensor.py:836\u001b[0m, in \u001b[0;36mTensor.linear\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight:Tensor, bias:Optional[Tensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 836\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmul(weight) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39madd(bias) \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/GitHub/aiforbattle/venv/lib/python3.9/site-packages/tinygrad/tensor.py:646\u001b[0m, in \u001b[0;36mTensor.dot\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    644\u001b[0m n1, n2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;28mlen\u001b[39m(w\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m n1 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n2 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth arguments to matmul need to be at least 1D, but they are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(n2, \u001b[38;5;241m2\u001b[39m)], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Tensor shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be multiplied (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(n2,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    647\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmin\u001b[39m(n1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    648\u001b[0m w \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmin\u001b[39m(n1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(n2, \u001b[38;5;241m2\u001b[39m):])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(n2, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input Tensor shapes (32768, 5) and (4, 3) cannot be multiplied (5 != 4)"
     ]
    }
   ],
   "source": [
    "# we write the quintic polynomial P(z)\n",
    "# (z-z1)(z-z2)(z-z3)(z-z4)(z-z5)\n",
    "\"\"\" \n",
    "Given a polynomial of degree 5: Vieta's formulas.\n",
    "$p(x) = x^5 + a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0$\n",
    "\n",
    "where the roots are r_1, r_2, r_3, r_4, r_5, the coefficients can be calculated as follows:\n",
    "\n",
    "- $a_4 = -(r_1 + r_2 + r_3 + r_4 + r_5)$\n",
    "- $a_3 = r_1r_2 + r_1r_3 + r_1r_4 + r_1r_5 + r_2r_3 + r_2r_4 + r_2r_5 + r_3r_4 + r_3r_5 + r_4r_5$\n",
    "- $a_2 = -(r_1r_2r_3 + r_1r_2r_4 + r_1r_2r_5 + r_1r_3r_4 + r_1r_3r_5 + r_1r_4r_5 + r_2r_3r_4 + r_2r_3r_5 + r_2r_4r_5 + r_3r_4r_5)$\n",
    "- $a_1 = r_1r_2r_3r_4 + r_1r_2r_3r_5 + r_1r_2r_4r_5 + r_1r_3r_4r_5 + r_2r_3r_4r_5$\n",
    "- $a_0 = -r_1r_2r_3r_4r_5$\n",
    "\n",
    "# Example usage\n",
    "coefficients = polynomial_coefficients(1, 2, 3, 4, 5)\n",
    "print(coefficients)\n",
    "\"\"\"\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "#function decaorator that prints arguments and function name\n",
    "def print_args(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        print(f\"called {func.__name__}\\n\"\n",
    "              f\"with args: {args}, kwargs: {kwargs}\")\n",
    "        return_value = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__} returns {return_value}\")\n",
    "        return return_value\n",
    "    return inner\n",
    "\n",
    "\n",
    "@print_args\n",
    "def polynomial_coefficients(roots: Tensor) -> List[Tensor]:\n",
    "    roots = [roots[:,i] for i in range(5)]\n",
    "    if len(roots) != 5:\n",
    "        raise ValueError(f\"Exactly 5 roots are required for a polynomial of degree 5., got {len(roots)}, got roots = {roots}\")\n",
    "    a4 = -sum(roots)\n",
    "    a3 = sum(r1*2 for r1, r2 in combinations(roots, 2))\n",
    "    a2 = -sum(r1*r2*r3 for r1, r2, r3 in combinations(roots, 3))\n",
    "    a1 = sum(r1*r2*r3*r4 for r1, r2, r3, r4 in combinations(roots, 4))\n",
    "    a0 = -roots[0]*roots[1]*roots[2]*roots[3]*roots[4]\n",
    "    return Tensor.stack([a4, a3, a2, a1, a0],dim=1)\n",
    "\n",
    "\n",
    "# step 1 train a neural net to learn the roots\n",
    "\n",
    "def train_model(model, lr:float = 0.01, steps:int = 1001, bs:int = 32768) -> Model:\n",
    "    opt = nn.optim.Adam(nn.state.get_parameters(model), lr)\n",
    "    lr_schedule = OneCycleLR(opt, max_lr=0.1, div_factor=100, final_div_factor=100, total_steps=steps, pct_start=0.5)\n",
    "    old_lr = opt.lr.numpy()\n",
    "    for i in range(steps):\n",
    "        roots = Tensor.rand(bs, 5).realize() # sample the roots\n",
    "        decoded_inputs = polynomial_coefficients(roots) # see if we can call this on a Tensor ? \n",
    "        loss = train_step(x=decoded_inputs, y=roots, model=model, opt=opt, lr_schedule=lr_schedule)\n",
    "        if i%100 == 0:\n",
    "            print(f\"lr = {opt.lr.numpy()[0]}\")\n",
    "            print(f\"loss at train_step = {i} : {loss.numpy()}\")\n",
    "    return model\n",
    "\n",
    "# step 3 apply it to a quintic polynomial\n",
    "\n",
    "model = train_model(Model(layers=25))\n",
    "  \n",
    "# step 4 use newton's method to find the roots given the initialization point given by the neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb",
   "language": "python",
   "name": "aifb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
