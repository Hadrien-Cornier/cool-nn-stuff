{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad import nn\n",
    "from tinygrad.nn.optim import Optimizer\n",
    "\n",
    "# from tinygrad.extra.lr_scheduler import LR_Scheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "DEGREE = 5 # degree of polynomial to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x: np.ndarray) -> Callable[[np.ndarray,str,str], None]:\n",
    "    \"\"\"\n",
    "    Curries X into the plot function\n",
    "    \"\"\"\n",
    "    def fn(y: np.ndarray, label: str, color: str) -> None:\n",
    "        plt.plot(x, y, label=label, color=color)\n",
    "    return lambda y,label,color : fn(y,label,color)\n",
    "\n",
    "\n",
    "def plt_setup(xlim:tuple = (0,1), ylim:tuple = (0,1), title:str = \"getting $a*(X^2)+b$ from Relus is the goal\") -> None:\n",
    "    plt.grid(True)\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.axvline(x=0, color='k')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(title)\n",
    "    plt.style.use('dark_background')\n",
    "    return None\n",
    "\n",
    "color_cycle = matplotlib.colormaps[\"Spectral\"]\n",
    "\n",
    "X=np.arange(0,1,0.001)\n",
    "p = plot(X)\n",
    "\n",
    "# from tinygrad.extra\n",
    "class LR_Scheduler:\n",
    "  def __init__(self, optimizer: Optimizer):\n",
    "    self.optimizer = optimizer\n",
    "    self.epoch_counter = Tensor([0], requires_grad=False, device=self.optimizer.device)\n",
    "\n",
    "  def get_lr(self): pass\n",
    "\n",
    "  def step(self) -> None:\n",
    "    self.epoch_counter.assign(self.epoch_counter + 1).realize()\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize()\n",
    "\n",
    "class OneCycleLR(LR_Scheduler):\n",
    "  def __init__(self, optimizer: Optimizer, max_lr: float, div_factor: float, final_div_factor: float, total_steps: int, pct_start: float):\n",
    "    super().__init__(optimizer)\n",
    "    self.initial_lr = max_lr / div_factor\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = self.initial_lr / final_div_factor\n",
    "    self.total_steps = total_steps\n",
    "    self.pct_start = pct_start\n",
    "    self.optimizer.lr.assign(self.get_lr()).realize() # update the initial LR\n",
    "\n",
    "  @staticmethod\n",
    "  def _annealing_linear(start: float, end: float, pct: Tensor) -> Tensor: return (pct*(end-start)+start)\n",
    "\n",
    "  def get_lr(self) -> Tensor:\n",
    "    return (self.epoch_counter < self.total_steps*self.pct_start).where(\n",
    "      self._annealing_linear(self.initial_lr, self.max_lr, self.epoch_counter/(self.total_steps*self.pct_start)),\n",
    "      self._annealing_linear(self.max_lr, self.min_lr, (self.epoch_counter-(self.total_steps*self.pct_start))/(self.total_steps*(1-self.pct_start)))\n",
    "    )\n",
    "    \n",
    "class Model:\n",
    "  # inputs : a, b, c, d, e (coefficients of polynomial)  -> actually just b,c,d,e because a is always 1\n",
    "  # expected output = roots z1, z2, z3, z4, z5 (assumed to be reals for now)\n",
    "\n",
    "  def __init__(self, layers:int = 3):\n",
    "    self.layers = [nn.Linear(4, 3), Tensor.relu] + [nn.Linear(3, 3), Tensor.relu]*layers + [nn.Linear(3, DEGREE), Tensor.sigmoid]\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor: return x.sequential(self.layers)\n",
    "\n",
    "  def L1(self) -> Tensor: return sum([l.weight.abs().sum() + l.bias.abs().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "  def L2(self) -> Tensor: return sum([l.weight.square().sum() + l.bias.square().sum() for l in self.layers if isinstance(l, nn.Linear)])\n",
    "\n",
    "\n",
    "def train_step(x:Tensor, y:Tensor, model: Model, opt: nn.optim.LAMB, lr_schedule: LR_Scheduler) -> Tensor:\n",
    "    y_pred = model(x)\n",
    "    loss = (y_pred - y).square().mean() #+ 0.0001 * (model.L2())# + 0.0001 * (model.L2())\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    lr_schedule.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def plot_model(model, title:str = \"neural network learning versus the takagi curve\") -> None:\n",
    "    #p(target(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"target\", color=color_cycle(0.0))\n",
    "    p(model(Tensor(X.astype(np.float32)).reshape(-1,1)).numpy(), \"model\", color=color_cycle(0.5))\n",
    "    #p(T_1(X)+T_2(X)+T_3(X), \"Takagi 3\", color=color_cycle(0.75))\n",
    "    plt_setup(xlim=(0,1), ylim=(0,1/2+0.1), title=title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "roots = [1,2,3,4,5]\n",
    "print(sum(r1 * r2 for r1, r2 in combinations(roots, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# step 3 apply it to a quintic polynomial\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# step 4 use newton's method to find the roots given the initialization point given by the neural network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, lr, steps, bs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m     12\u001b[0m     samples \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mrand(bs, DEGREE)\u001b[38;5;241m.\u001b[39mrealize() \u001b[38;5;66;03m# sample the roots\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m(samples)\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(samples, y, model, opt, lr_schedule)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "# we write the quintic polynomial P(z)\n",
    "# (z-z1)(z-z2)(z-z3)(z-z4)(z-z5)\n",
    "\"\"\" \n",
    "Given a polynomial of degree 5:\n",
    "Vieta's formulas.\n",
    "$\\[ p(x) = x^5 + a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0 \\]$\n",
    "\n",
    "where the roots are \\( r_1, r_2, r_3, r_4, r_5 \\), the coefficients can be calculated as follows:\n",
    "\n",
    "- $\\( a_4 = -(r_1 + r_2 + r_3 + r_4 + r_5) \\)$\n",
    "- $\\( a_3 = r_1r_2 + r_1r_3 + r_1r_4 + r_1r_5 + r_2r_3 + r_2r_4 + r_2r_5 + r_3r_4 + r_3r_5 + r_4r_5 \\)$\n",
    "- $\\( a_2 = -(r_1r_2r_3 + r_1r_2r_4 + r_1r_2r_5 + r_1r_3r_4 + r_1r_3r_5 + r_1r_4r_5 + r_2r_3r_4 + r_2r_3r_5 + r_2r_4r_5 + r_3r_4r_5) \\)$\n",
    "- $\\( a_1 = r_1r_2r_3r_4 + r_1r_2r_3r_5 + r_1r_2r_4r_5 + r_1r_3r_4r_5 + r_2r_3r_4r_5 \\)$\n",
    "- $\\( a_0 = -r_1r_2r_3r_4r_5 \\)$\n",
    "\n",
    "# Example usage\n",
    "coefficients = polynomial_coefficients(1, 2, 3, 4, 5)\n",
    "print(coefficients)\n",
    "\"\"\"\n",
    "from itertools import combinations\n",
    "\n",
    "def polynomial_coefficients(*roots):\n",
    "    if len(roots) != 5:\n",
    "        raise ValueError(\"Exactly 5 roots are required for a polynomial of degree 5.\")\n",
    "    a4 = -sum(roots)\n",
    "    a3 = sum(r1 * r2 for r1, r2 in combinations(roots, 2))\n",
    "    a2 = -sum(r1 * r2 * r3 for r1, r2, r3 in combinations(roots, 3))\n",
    "    a1 = sum(r1 * r2 * r3 * r4 for r1, r2, r3, r4 in combinations(roots, 4))\n",
    "    a0 = -roots[0] * roots[1] * roots[2] * roots[3] * roots[4]\n",
    "    return [1, a4, a3, a2, a1, a0]\n",
    "\n",
    "\n",
    "# step 1 train a neural net to learn the roots\n",
    "\n",
    "def train_model(model, lr:float = 0.01, steps:int = 1001, bs:int = 32768) -> Model:\n",
    "    opt = nn.optim.Adam(nn.state.get_parameters(model), lr)\n",
    "    lr_schedule = OneCycleLR(opt, max_lr=0.1, div_factor=100, final_div_factor=100, total_steps=steps, pct_start=0.5)\n",
    "    old_lr = opt.lr.numpy()\n",
    "    for i in range(steps):\n",
    "        roots = Tensor.rand(bs, DEGREE).realize() # sample the roots\n",
    "        decoded_inputs = polynomial_coefficients(roots) # see if we can call this on a Tensor ? \n",
    "        loss = train_step(x=decode_inputs, y=roots, model, opt, lr_schedule)\n",
    "        if i%100 == 0:\n",
    "            print(f\"lr = {opt.lr.numpy()[0]}\")\n",
    "            print(f\"loss at train_step = {i} : {loss.numpy()}\")\n",
    "    return model\n",
    "\n",
    "# step 3 apply it to a quintic polynomial\n",
    "\n",
    "model = train_model(Model(layers=25))\n",
    "  \n",
    "# step 4 use newton's method to find the roots given the initialization point given by the neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifb",
   "language": "python",
   "name": "aifb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
