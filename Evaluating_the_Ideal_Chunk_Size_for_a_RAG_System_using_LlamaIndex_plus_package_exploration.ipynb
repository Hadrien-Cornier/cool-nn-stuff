{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hadrien-Cornier/cool-nn-stuff/blob/main/Evaluating_the_Ideal_Chunk_Size_for_a_RAG_System_using_LlamaIndex_plus_package_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
      ],
      "metadata": {
        "id": "9FqeieOC5vUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes handy. In this blogpost, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex’s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding."
      ],
      "metadata": {
        "id": "NIvSXj365r2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why Chunk Size Matters**\n",
        "\n",
        "Choosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
        "\n",
        "1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "\n",
        "In essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
      ],
      "metadata": {
        "id": "dpbtWrEa53Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**\n",
        "\n",
        "Before embarking on the experiment, we need to ensure all requisite modules are imported:"
      ],
      "metadata": {
        "id": "SR8jlf3358_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ItNWVKRRD67j",
        "outputId": "68251874-d669-4bfc-e780-3c5def41acf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.30-py3-none-any.whl (6.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.2-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.30 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.30-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.8-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.16-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.5-py3-none-any.whl (4.1 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.19-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
            "  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.18-py3-none-any.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (4.66.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.1-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.7.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.30->llama-index) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-0.10.30 llama-index-agent-openai-0.2.2 llama-index-cli-0.1.12 llama-index-core-0.10.30 llama-index-embeddings-openai-0.1.8 llama-index-indices-managed-llama-cloud-0.1.5 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.16 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.5 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.19 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.1 llamaindex-py-client-0.1.18 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.23.2 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index"
      ],
      "metadata": {
        "id": "G-W7UqhWFPMU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama_index.__spec__.origin)"
      ],
      "metadata": {
        "id": "TMtMhXcPFgXK",
        "outputId": "75b9676e-cbc0-47bc-d4d6-0b6b3062122b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama_index.__version__)"
      ],
      "metadata": {
        "id": "-HuDP-SEFYaG",
        "outputId": "9f810b89-333e-4f5f-e707-247ee8e0c5af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'llama_index' has no attribute '__version__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-54456d86a584>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'llama_index' has no attribute '__version__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.readers import SimpleDirectoryReader\n",
        "from llama_index.core.vector_stores import SimpleVectorStore"
      ],
      "metadata": {
        "id": "AV_hW_TqGH_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip show llama_index"
      ],
      "metadata": {
        "id": "595GyEVUIwB_",
        "outputId": "19cad2da-120c-4c99-b242-afba83d1daca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: llama-index\n",
            "Version: 0.10.30\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: Jerry Liu\n",
            "Author-email: jerry@llamaindex.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import importlib.util\n",
        "\n",
        "def is_package(path):\n",
        "    \"\"\" Check if a given path is a Python package \"\"\"\n",
        "    init_py = os.path.join(path, '__init__.py')\n",
        "    return os.path.isfile(init_py)\n",
        "\n",
        "def list_python_files(directory):\n",
        "    \"\"\" List python files in a directory excluding __init__.py \"\"\"\n",
        "    files = [f[:-3] for f in os.listdir(directory) if f.endswith('.py') and f != '__init__.py']\n",
        "    return files\n",
        "\n",
        "def generate_import_statements(package_path):\n",
        "    \"\"\" Generate import statements for all sub-packages and modules in the given package directory \"\"\"\n",
        "    for root, dirs, files in os.walk(package_path):\n",
        "        if is_package(root):\n",
        "            # Relative path of the package from the root package path\n",
        "            relative_path = os.path.relpath(root, package_path).replace(os.path.sep, '.')\n",
        "            # List of importable Python files in the directory\n",
        "            modules = list_python_files(root)\n",
        "            if modules:\n",
        "                print(f\"from {relative_path} import {', '.join(modules)}\")\n",
        "\n",
        "# Example usage:\n",
        "package_path = '/usr/local/lib/python3.10/dist-packages/llama_index'  # Change this to the path of your package\n",
        "generate_import_statements(package_path)"
      ],
      "metadata": {
        "id": "MjdV9D9fIeHX",
        "outputId": "a6231d21-3f24-4c54-bc22-882c193352c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from program.openai import base\n",
            "from multi_modal_llms.openai import utils, base\n",
            "from indices.managed.llama_cloud import retriever, base\n",
            "from llms.openai import utils, base\n",
            "from core import image_retriever, utils, async_utils, schema, settings, exec_utils, service_context, img_utils, types, constants\n",
            "from core.callbacks import llama_debug, utils, base, simple_llm_handler, pythonically_printing_base_handler, schema, global_handlers, base_handler, token_counting\n",
            "from core.vector_stores import utils, types, simple\n",
            "from core.playground import base\n",
            "from core.composability import base, joint_qa_summary\n",
            "from core.bridge import langchain, pydantic\n",
            "from core.extractors import interface, metadata_extractors, loading\n",
            "from core.program import utils, multi_modal_llm_program, llm_program, llm_prompt_program\n",
            "from core.multi_modal_llms import base, generic_utils\n",
            "from core.postprocessor import optimizer, pii, metadata_replacement, sbert_rerank, llm_rerank, node, rankGPT_rerank, types, node_recency\n",
            "from core.download import utils, module, dataset, pack, integration\n",
            "from core.evaluation import context_relevancy, faithfulness, guideline, answer_relevancy, base, relevancy, batch_runner, notebook_utils, dataset_generation, correctness, pairwise, semantic_similarity, eval_utils\n",
            "from core.evaluation.benchmarks import beir, hotpotqa\n",
            "from core.evaluation.multi_modal import faithfulness, relevancy\n",
            "from core.evaluation.retrieval import metrics, base, metrics_base, evaluator\n",
            "from core.output_parsers import utils, base, langchain, selection, pydantic\n",
            "from core.utilities import sql_wrapper, aws_utils, token_counting, gemini_utils\n",
            "from core.tools import download, utils, query_plan, calling, function_tool, retriever_tool, ondemand_loader_tool, eval_query_engine, query_engine, types\n",
            "from core.tools.tool_spec import base\n",
            "from core.tools.tool_spec.load_and_search import base\n",
            "from core.query_engine import multistep_query_engine, retry_query_engine, cogniswitch_query_engine, jsonalyze_query_engine, custom, retriever_query_engine, transform_query_engine, sql_join_query_engine, retry_source_query_engine, sub_question_query_engine, citation_query_engine, knowledge_graph_query_engine, graph_query_engine, multi_modal, sql_vector_query_engine, router_query_engine\n",
            "from core.query_engine.flare import base, answer_inserter, schema, output_parser\n",
            "from core.query_engine.pandas import pandas_query_engine, output_parser\n",
            "from core.base import base_query_engine, base_retriever, base_auto_retriever, base_selector, base_multi_modal_retriever\n",
            "from core.base.llms import base, generic_utils, types\n",
            "from core.base.response import schema\n",
            "from core.base.embeddings import base\n",
            "from core.base.query_pipeline import query\n",
            "from core.storage import storage_context\n",
            "from core.storage.index_store import simple_index_store, utils, postgres_index_store, keyval_index_store, types\n",
            "from core.storage.chat_store import simple_chat_store, base, loading\n",
            "from core.storage.docstore import utils, registry, postgres_docstore, simple_docstore, types, keyval_docstore\n",
            "from core.storage.kvstore import postgres_kvstore, simple_kvstore, types\n",
            "from core.retrievers import auto_merging_retriever, recursive_retriever, transform_retriever, fusion_retriever, router_retriever\n",
            "from core.chat_engine import context, utils, condense_plus_context, types, condense_question, simple\n",
            "from core.command_line import upgrade\n",
            "from core.indices import utils, base, registry, service_context, base_retriever, postprocessor, prompt_helper, loading\n",
            "from core.indices.query import base, schema, embedding_utils\n",
            "from core.indices.query.query_transform import feedback_transform, base, prompts\n",
            "from core.indices.composability import graph\n",
            "from core.indices.knowledge_graph import base, retrievers\n",
            "from core.indices.tree import utils, tree_root_retriever, base, select_leaf_retriever, select_leaf_embedding_retriever, inserter, all_leaf_retriever\n",
            "from core.indices.list import base, retrievers\n",
            "from core.indices.document_summary import base, retrievers\n",
            "from core.indices.vector_store import base\n",
            "from core.indices.vector_store.retrievers import retriever\n",
            "from core.indices.vector_store.retrievers.auto_retriever import auto_retriever, output_parser, prompts\n",
            "from core.indices.multi_modal import retriever, base\n",
            "from core.indices.common.struct_store import base, sql, schema\n",
            "from core.indices.empty import base, retrievers\n",
            "from core.indices.struct_store import pandas, base, sql, json_query, container_builder, sql_query, sql_retriever\n",
            "from core.indices.keyword_table import utils, base, simple_base, retrievers, rake_base\n",
            "from core.indices.common_tree import base\n",
            "from core.indices.managed import base, types\n",
            "from core.ingestion import pipeline, api_utils, cache, transformations, data_sources, data_sinks\n",
            "from core.service_context_elements import llm_predictor, llama_logger\n",
            "from core.llms import chatml_utils, callbacks, utils, custom, mock, function_calling, loading, llm\n",
            "from core.response import utils, notebook_utils, pprint_utils\n",
            "from core.prompts import chat_prompts, prompt_type, system, default_prompt_selectors, utils, base, default_prompts, display_utils, mixin, guidance_utils, prompts, prompt_utils\n",
            "from core.llama_dataset import download, base, generator, evaluator_evaluation, rag, simple\n",
            "from core.llama_dataset.legacy import embedding\n",
            "from core.agent import utils, types\n",
            "from core.agent.function_calling import base, step\n",
            "from core.agent.runner import base, parallel\n",
            "from core.agent.custom import pipeline_worker, simple\n",
            "from core.agent.react import formatter, base, step, output_parser, types, agent, prompts\n",
            "from core.agent.legacy.react import base\n",
            "from core.agent.react_multimodal import step, prompts\n",
            "from core.langchain_helpers import streaming, text_splitter, memory_wrapper\n",
            "from core.langchain_helpers.agents import tools, agents, toolkits\n",
            "from core.memory import chat_memory_buffer, types\n",
            "from core.response_synthesizers import simple_summarize, factory, base, generation, accumulate, tree_summarize, type, compact_and_refine, refine, no_text, compact_and_accumulate\n",
            "from core.graph_stores import types, simple\n",
            "from core.llama_pack import download, base\n",
            "from core.readers import string_iterable, download, base, json, loading\n",
            "from core.readers.file import base\n",
            "from core.question_gen import llm_generators, output_parser, types, prompts\n",
            "from core.embeddings import pooling, utils, mock_embed_model, loading, multi_modal_base\n",
            "from core.instrumentation import dispatcher\n",
            "from core.instrumentation.span_handlers import base, null, simple\n",
            "from core.instrumentation.events import base, synthesis, retrieval, embedding, span, chat_engine, agent, query, llm\n",
            "from core.instrumentation.span import base, simple\n",
            "from core.instrumentation.event_handlers import base, null\n",
            "from core.query_pipeline import query\n",
            "from core.query_pipeline.components import router, input, argpacks, function, agent, tool_runner\n",
            "from core.node_parser import interface, node_utils, loading\n",
            "from core.node_parser.text import sentence, utils, sentence_window, langchain, semantic_splitter, token, code\n",
            "from core.node_parser.relational import markdown_element, base_element, utils, hierarchical, unstructured_element, llama_parse_json_element\n",
            "from core.node_parser.file import simple_file, markdown, json, html\n",
            "from core.data_structs import document_summary, data_structs, registry, struct_type, table\n",
            "from core.selectors import utils, pydantic_selectors, llm_selectors, embedding_selectors, types, prompts\n",
            "from core.objects import table_node_mapping, utils, base, tool_node_mapping, fn_node_mapping, base_node_mapping\n",
            "from cli import command_line\n",
            "from cli.rag import base\n",
            "from cli.upgrade import base\n",
            "from cli.new_package import base\n",
            "from cli.new_package.templates import readme, init, pyproject\n",
            "from agent.openai import utils, base, openai_assistant_agent, step\n",
            "from readers.file.unstructured import base\n",
            "from readers.file.flat import base\n",
            "from readers.file.markdown import base\n",
            "from readers.file.image_caption import base\n",
            "from readers.file.xml import base\n",
            "from readers.file.paged_csv import base\n",
            "from readers.file.html import base\n",
            "from readers.file.image_deplot import base\n",
            "from readers.file.mbox import base\n",
            "from readers.file.slides import base\n",
            "from readers.file.rtf import base\n",
            "from readers.file.docs import base\n",
            "from readers.file.tabular import base\n",
            "from readers.file.video_audio import base\n",
            "from readers.file.epub import base\n",
            "from readers.file.image import base\n",
            "from readers.file.ipynb import base\n",
            "from readers.file.pymu_pdf import base\n",
            "from readers.file.image_vision_llm import base\n",
            "from question_gen.openai import base\n",
            "from embeddings.openai import utils, base\n",
            "from legacy import utils, async_utils, schema, exec_utils, service_context, img_utils, types, constants\n",
            "from legacy.callbacks import llama_debug, utils, base, honeyhive_callback, argilla_callback, arize_phoenix_callback, deepeval_callback, simple_llm_handler, wandb_callback, schema, global_handlers, base_handler, finetuning_handler, open_inference_callback, promptlayer_handler, token_counting, aim\n",
            "from legacy.vector_stores import azureaisearch, faiss, awadb, timescalevector, pinecone, pgvecto_rs, weaviate_utils, bagel, utils, chatgpt_plugin, epsilla, dashvector, tencentvectordb, deeplake, milvus, singlestoredb, neo4jvector, azurecosmosmongo, astra, myscale, cassandra, lancedb, qdrant_utils, registry, redis, supabase, pinecone_utils, weaviate, tair, chroma, upstash, txtai, elasticsearch, dynamodb, lantern, loading, postgres, typesense, qdrant, jaguar, metal, zep, types, rocksetdb, mongodb, opensearch, simple\n",
            "from legacy.vector_stores.google.generativeai import genai_extension, base\n",
            "from legacy.vector_stores.docarray import base, hnsw, in_memory\n",
            "from legacy.playground import base\n",
            "from legacy.composability import base, joint_qa_summary\n",
            "from legacy.bridge import langchain, pydantic\n",
            "from legacy.extractors import interface, marvin_metadata_extractor, metadata_extractors, loading\n",
            "from legacy.program import utils, multi_modal_llm_program, lmformatenforcer_program, llm_program, llm_prompt_program, openai_program, guidance_program\n",
            "from legacy.program.predefined import df\n",
            "from legacy.program.predefined.evaporate import base, extractor, prompts\n",
            "from legacy.multi_modal_llms import openai, dashscope, base, dashscope_utils, azure_openai, generic_utils, ollama, gemini, replicate_multi_modal, openai_utils\n",
            "from legacy.postprocessor import cohere_rerank, flag_embedding_reranker, optimizer, pii, metadata_replacement, sbert_rerank, llm_rerank, node, rankGPT_rerank, longllmlingua, types, node_recency\n",
            "from legacy.token_counter import utils, mock_embed_model\n",
            "from legacy.download import utils, module, dataset\n",
            "from legacy.evaluation import context_relevancy, faithfulness, guideline, answer_relevancy, base, relevancy, batch_runner, notebook_utils, dataset_generation, correctness, pairwise, semantic_similarity, eval_utils\n",
            "from legacy.evaluation.benchmarks import beir, hotpotqa\n",
            "from legacy.evaluation.multi_modal import faithfulness, relevancy\n",
            "from legacy.evaluation.retrieval import metrics, base, metrics_base, evaluator\n",
            "from legacy.llm_predictor import base, structured, mock, loading\n",
            "from legacy.llm_predictor.vellum import exceptions, utils, predictor, prompt_registry, types\n",
            "from legacy.output_parsers import guardrails, utils, base, langchain, selection, pydantic\n",
            "from legacy.utilities import sql_wrapper, aws_utils, token_counting\n",
            "from legacy.tools import download, utils, query_plan, function_tool, retriever_tool, ondemand_loader_tool, query_engine, types\n",
            "from legacy.tools.tool_spec import base\n",
            "from legacy.tools.tool_spec.notion import base\n",
            "from legacy.tools.tool_spec.load_and_search import base\n",
            "from legacy.tools.tool_spec.slack import base\n",
            "from legacy.query_engine import multistep_query_engine, retry_query_engine, cogniswitch_query_engine, jsonalyze_query_engine, custom, retriever_query_engine, transform_query_engine, sql_join_query_engine, retry_source_query_engine, sub_question_query_engine, citation_query_engine, knowledge_graph_query_engine, graph_query_engine, multi_modal, sql_vector_query_engine, router_query_engine\n",
            "from legacy.query_engine.flare import base, answer_inserter, schema, output_parser\n",
            "from legacy.query_engine.pandas import pandas_query_engine, output_parser\n",
            "from legacy.storage import storage_context\n",
            "from legacy.storage.index_store import simple_index_store, utils, mongo_index_store, firestore_indexstore, postgres_index_store, dynamodb_index_store, redis_index_store, keyval_index_store, types\n",
            "from legacy.storage.chat_store import simple_chat_store, base, redis_chat_store, loading\n",
            "from legacy.storage.docstore import mongo_docstore, dynamodb_docstore, utils, firestore_docstore, registry, postgres_docstore, simple_docstore, redis_docstore, types, keyval_docstore\n",
            "from legacy.storage.kvstore import mongodb_kvstore, redis_kvstore, postgres_kvstore, s3_kvstore, simple_kvstore, dynamodb_kvstore, firestore_kvstore, types\n",
            "from legacy.retrievers import auto_merging_retriever, recursive_retriever, pathway_retriever, transform_retriever, you_retriever, fusion_retriever, router_retriever, bm25_retriever\n",
            "from legacy.chat_engine import context, utils, condense_plus_context, types, condense_question, simple\n",
            "from legacy.command_line import command_line, rag\n",
            "from legacy.indices import utils, base, registry, service_context, base_retriever, postprocessor, prompt_helper, loading\n",
            "from legacy.indices.query import base, schema, embedding_utils\n",
            "from legacy.indices.query.query_transform import feedback_transform, base, prompts\n",
            "from legacy.indices.composability import graph\n",
            "from legacy.indices.knowledge_graph import base, retrievers\n",
            "from legacy.indices.tree import utils, tree_root_retriever, base, select_leaf_retriever, select_leaf_embedding_retriever, inserter, all_leaf_retriever\n",
            "from legacy.indices.list import base, retrievers\n",
            "from legacy.indices.document_summary import base, retrievers\n",
            "from legacy.indices.vector_store import base\n",
            "from legacy.indices.vector_store.retrievers import retriever\n",
            "from legacy.indices.vector_store.retrievers.auto_retriever import auto_retriever, output_parser, prompts\n",
            "from legacy.indices.multi_modal import retriever, base\n",
            "from legacy.indices.common.struct_store import base, sql, schema\n",
            "from legacy.indices.empty import base, retrievers\n",
            "from legacy.indices.struct_store import pandas, base, sql, json_query, container_builder, sql_query, sql_retriever\n",
            "from legacy.indices.keyword_table import utils, base, simple_base, retrievers, rake_base\n",
            "from legacy.indices.common_tree import base\n",
            "from legacy.indices.managed import base, types\n",
            "from legacy.indices.managed.google.generativeai import base\n",
            "from legacy.indices.managed.zilliz import retriever, base\n",
            "from legacy.indices.managed.colbert_index import retriever, base\n",
            "from legacy.indices.managed.vectara import retriever, base, query, prompts\n",
            "from legacy.ingestion import pipeline, cache\n",
            "from legacy.llms import nvidia_triton_utils, nvidia_triton, neutrino, sagemaker_llm_endpoint, vllm, llama_cpp, together, utils, replicate, openai, everlyai_utils, konko, portkey, perplexity, cohere, custom, dashscope, palm, localai, litellm_utils, base, langchain, anthropic, dashscope_utils, openllm, konko_utils, mock, ai21, huggingface, sagemaker_llm_endpoint_utils, langchain_utils, mistralai_utils, bedrock_utils, azure_openai, llama_api, clarifai, generic_utils, vllm_utils, ollama, anyscale, watsonx, vertex_gemini_utils, ai21_utils, openrouter, openai_like, anyscale_utils, everlyai, gemini, llama_utils, vertex_utils, anthropic_utils, portkey_utils, cohere_utils, nvidia_tensorrt_utils, xinference_utils, monsterapi, vertex, loading, predibase, mistral, rungpt, gradient, litellm, types, watsonx_utils, openai_utils, xinference, llm, bedrock, nvidia_tensorrt, gemini_utils\n",
            "from legacy.logger import base\n",
            "from legacy.response import utils, notebook_utils, schema, pprint_utils\n",
            "from legacy.core import image_retriever, base_query_engine, base_retriever, base_auto_retriever, base_selector, base_multi_modal_retriever\n",
            "from legacy.core.llms import types\n",
            "from legacy.core.response import schema\n",
            "from legacy.core.embeddings import base\n",
            "from legacy.core.query_pipeline import query_component, components\n",
            "from legacy.prompts import chat_prompts, prompt_type, system, default_prompt_selectors, lmformatenforcer_utils, utils, base, default_prompts, display_utils, mixin, guidance_utils, prompts, prompt_utils\n",
            "from legacy.llama_dataset import download, base, generator, evaluator_evaluation, rag\n",
            "from legacy.agent import utils, openai_assistant_agent, types\n",
            "from legacy.agent.openai import utils, base, step\n",
            "from legacy.agent.runner import base, parallel\n",
            "from legacy.agent.custom import pipeline_worker, simple\n",
            "from legacy.agent.react import formatter, base, step, output_parser, types, agent, prompts\n",
            "from legacy.agent.legacy import context_retriever_agent, openai_agent, retriever_openai_agent\n",
            "from legacy.agent.legacy.react import base\n",
            "from legacy.agent.react_multimodal import step, prompts\n",
            "from legacy.langchain_helpers import streaming, text_splitter, memory_wrapper\n",
            "from legacy.langchain_helpers.agents import tools, agents, toolkits\n",
            "from legacy.memory import chat_memory_buffer, types\n",
            "from legacy.response_synthesizers import simple_summarize, factory, base, generation, accumulate, tree_summarize, type, compact_and_refine, refine, no_text, compact_and_accumulate\n",
            "from legacy.response_synthesizers.google.generativeai import base\n",
            "from legacy.param_tuner import base\n",
            "from legacy.graph_stores import falkordb, nebulagraph, neo4j, registry, kuzu, types, simple\n",
            "from legacy.llama_pack import download, base\n",
            "from legacy.tts import base, elevenlabs, bark\n",
            "from legacy.readers import string_iterable, notion, download, faiss, awadb, pinecone, slack, bagel, pathway, dashvector, youtube_transcript, web, base, deeplake, milvus, wikipedia, myscale, mbox, twitter, database, chroma, txtai, elasticsearch, json, discord_reader, psychic, mongo, obsidian, loading, qdrant, jaguar, metal\n",
            "from legacy.readers.redis import utils\n",
            "from legacy.readers.steamship import file_reader\n",
            "from legacy.readers.make_com import wrapper\n",
            "from legacy.readers.file import markdown_reader, docs_reader, epub_reader, image_caption_reader, image_reader, tabular_reader, ipynb_reader, base, video_audio_reader, mbox_reader, image_vision_llm_reader, slides_reader, flat_reader, html_reader\n",
            "from legacy.readers.schema import base\n",
            "from legacy.readers.weaviate import reader\n",
            "from legacy.readers.google_readers import gdocs, gsheets\n",
            "from legacy.readers.github_readers import github_api_client, utils, github_repository_reader\n",
            "from legacy.readers.chatgpt_plugin import base\n",
            "from legacy.question_gen import guidance_generator, llm_generators, output_parser, openai_generator, types, prompts\n",
            "from legacy.embeddings import voyageai, pooling, nomic, together, ollama_embedding, utils, openai, adapter, text_embeddings_inference, dashscope, base, sagemaker_embedding_endpoint_utils, langchain, huggingface_optimum, sagemaker_embedding_endpoint, huggingface, azure_openai, clarifai, adapter_utils, anyscale, jinaai, elasticsearch, gemini, huggingface_utils, llm_rails, clip, instructor, loading, fastembed, google, gradient, google_palm, multi_modal_base, mistralai, cohereai, bedrock\n",
            "from legacy.finetuning import types\n",
            "from legacy.finetuning.openai import validate_json, base\n",
            "from legacy.finetuning.gradient import base\n",
            "from legacy.finetuning.cross_encoders import dataset_gen, cross_encoder\n",
            "from legacy.finetuning.rerankers import dataset_gen, cohere_reranker\n",
            "from legacy.finetuning.embeddings import adapter, sentence_transformer, adapter_utils, common\n",
            "from legacy.query_pipeline import query\n",
            "from legacy.query_pipeline.components import router, agent, tool_runner\n",
            "from legacy.node_parser import interface, node_utils, loading\n",
            "from legacy.node_parser.text import sentence, utils, sentence_window, langchain, semantic_splitter, token, code\n",
            "from legacy.node_parser.relational import markdown_element, base_element, hierarchical, unstructured_element\n",
            "from legacy.node_parser.file import simple_file, markdown, json, html\n",
            "from legacy.data_structs import document_summary, data_structs, registry, struct_type, table\n",
            "from legacy.selectors import utils, pydantic_selectors, llm_selectors, embedding_selectors, prompts\n",
            "from legacy.objects import table_node_mapping, base, tool_node_mapping, base_node_mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvV4DO16IjbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = '' # set your openai api key"
      ],
      "metadata": {
        "id": "y9SVm76h58de",
        "outputId": "413b9403-aa61-4c63-f8d9-e7c151362e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'SimpleDirectoryReader' from 'llama_index' (unknown location)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1462b7b95f73>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from llama_index import (\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SimpleDirectoryReader' from 'llama_index' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download Data**\n",
        "\n",
        "We'll be using the Uber 10K SEC Filings for 2021 for this experiment."
      ],
      "metadata": {
        "id": "SvEzZzif6G5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZOD_9THEErrc",
        "outputId": "313879d2-e2be-44c8-e72e-584682ad7b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 19:59:18--  https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-04-20 19:59:18 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/10k/'\n",
        "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Data**\n",
        "\n",
        "Let’s load our document."
      ],
      "metadata": {
        "id": "bO21UssT6L8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "\n",
        "reader = SimpleDirectoryReader(\"./data/10k/\")\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "id": "x6QdEBd-17OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question Generation**\n",
        "\n",
        "To select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents."
      ],
      "metadata": {
        "id": "jnpPtiz56TYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
        "eval_documents = documents[:20]\n",
        "data_generator = DatasetGenerator.from_documents()\n",
        "eval_questions = data_generator.generate_questions_from_nodes(num = 40)"
      ],
      "metadata": {
        "id": "26BgDF3L6Z0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Evaluators\n",
        "\n",
        "We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised with the `service_context` .\n",
        "\n",
        "1. **Faithfulness Evaluator** - It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
        "2. **Relevancy Evaluator** - It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query."
      ],
      "metadata": {
        "id": "C3WwA-0N6dMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use GPT-4 for evaluating the responses\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# Define service context for GPT-4 for evaluation\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
        "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n"
      ],
      "metadata": {
        "id": "G2LoMRtr6fnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Response Evaluation For A Chunk Size**\n",
        "\n",
        "We evaluate each chunk_size based on 3 metrics.\n",
        "\n",
        "1. Average Response Time.\n",
        "2. Average Faithfulness.\n",
        "3. Average Relevancy.\n",
        "\n",
        "Here's a function, `evaluate_response_time_and_accuracy`, that does just that which has:\n",
        "\n",
        "1. VectorIndex Creation.\n",
        "2. Building the Query Engine**.**\n",
        "3. Metrics Calculation."
      ],
      "metadata": {
        "id": "UUncIIxR6gVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # create vector index\n",
        "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
        "    vector_index = VectorStoreIndex.from_documents(\n",
        "        eval_documents, service_context=service_context\n",
        "    )\n",
        "    # build query engine\n",
        "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "    query_engine = vector_index.as_query_engine()\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "    for question in eval_questions:\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ],
      "metadata": {
        "id": "dEC2Lr0z6p1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing Across Different Chunk Sizes**\n",
        "\n",
        "We'll evaluate a range of chunk sizes to identify which offers the most promising metrics"
      ],
      "metadata": {
        "id": "p8DQvTP96s48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
        "\n",
        "for chunk_size in [128, 256, 512, 1024, 2048]:\n",
        "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n",
        "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
      ],
      "metadata": {
        "id": "jlKICwXH6Tib"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}